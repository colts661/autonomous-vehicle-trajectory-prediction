{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e33dfb",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "\n",
    "Upon building the final model we use for the result of the competition, we consulted [TensorFlow tutorials](https://www.tensorflow.org/text/tutorials/transformer) and PyTorch tutorials with practical examples such as [Language Modeling](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) and [Language Translation](https://pytorch.org/tutorials/beginner/translation_transformer.html.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a4e38",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3d7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e3292",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6db7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def transform_data(np_data, bch_id):\n",
    "    df = pd.DataFrame(np_data[bch_id], columns = ['x','y'])\n",
    "    df['x_vel'] = np.gradient(df.x)\n",
    "    df['y_vel'] = np.gradient(df.y)\n",
    "    df['vel'] = np.sqrt(df.x_vel**2 + df.y_vel**2)\n",
    "    df['x_acc'] = np.gradient(df.x_vel)\n",
    "    df['y_acc'] = np.gradient(df.y_vel)\n",
    "    df['acc'] = np.gradient(df.vel)\n",
    "    tangent = np.array([1/df.vel]*2).T * np.array([df.x_vel, df.y_vel]).T\n",
    "    df['curvature'] = np.abs(df.x_acc * df.y_vel - df.x_vel * df.y_acc) / (df.vel)**3\n",
    "    out = df[['x', 'y', 'curvature']]\n",
    "    return out.to_numpy()\n",
    "\n",
    "\n",
    "def rotate(X, startpoint, endpoint, default_angle):\n",
    "    \n",
    "    # Find the slope of the path\n",
    "    dx = X[:, endpoint, 0] - X[:, startpoint, 0]\n",
    "    dy = X[:, endpoint, 1] - X[:, startpoint, 1]\n",
    "    \n",
    "    # Convert theta to degree in the range(0, 360)\n",
    "    theta = np.arctan2(dy, dx)\n",
    "    angle = np.degrees(theta)\n",
    "    angle[angle < 0] += 360\n",
    "    \n",
    "    # Generate the degree we want to rotate by and convert back to theta\n",
    "    rotate_degree = -1 * (angle - default_angle)\n",
    "    rotate_theta = np.deg2rad(rotate_degree)\n",
    "    \n",
    "    # Reshape the array from [4, batchsize] to [batchsize, 2, 2]\n",
    "    rot = np.array([np.cos(rotate_theta), -np.sin(rotate_theta),\n",
    "                np.sin(rotate_theta), np.cos(rotate_theta)])\n",
    "    rot = rot.T.reshape(-1, 2, 2)\n",
    "    \n",
    "    return rot\n",
    "\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None, normalized=False):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.normalized = normalized\n",
    "        self.split = split\n",
    "\n",
    "        self.inputs, self.outputs = self.get_city_trajectories(city=city, split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == 'train':\n",
    "        \n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "\n",
    "#             if self.transform:\n",
    "#                 data = self.transform(data)\n",
    "\n",
    "            return data\n",
    "        \n",
    "        return self.inputs[idx]\n",
    "    \n",
    "    def get_city_trajectories(self, city=\"palo-alto\", split=\"train\"):\n",
    "        assert city in cities and split in splits\n",
    "\n",
    "        # get input\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "        # store input starting positions and rotation matrix\n",
    "        start_pos = inputs[:, 0, :].copy()\n",
    "        rotate_factor = rotate(inputs, 0, 49, 30)\n",
    "        max_factor = inputs.max(axis=1)\n",
    "        \n",
    "#         print(inputs.reshape(-1, 2).mean(axis=0))\n",
    "#         print(inputs.reshape(-1, 2).std(axis=0))\n",
    "\n",
    "        # normalize inputs (translation + rotation)\n",
    "        if self.normalized:\n",
    "            for i in range(len(inputs)):\n",
    "                inputs[i] -= start_pos[i, :]\n",
    "                \n",
    "            for i in range(len(inputs)):\n",
    "                inputs[i] = inputs[i] @ rotate_factor[i].T\n",
    "            \n",
    "            max_factor = inputs.max(axis=1)\n",
    "            \n",
    "#             for i in range(len(inputs)):\n",
    "#                 inputs[i] = inputs[i] / max_factor[i]\n",
    "\n",
    "        # get output\n",
    "        outputs = None\n",
    "        if split == \"train\":  # get and normalize outputs\n",
    "            f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "            outputs = pickle.load(open(f_out, \"rb\"))\n",
    "            outputs = np.asarray(outputs)\n",
    "            if self.normalized:\n",
    "                for i in range(len(inputs)):\n",
    "                    outputs[i] -= start_pos[i, :]\n",
    "                    \n",
    "                for i in range(len(inputs)):\n",
    "                    outputs[i] = outputs[i] @ rotate_factor[i].T\n",
    "                \n",
    "#                 for i in range(len(inputs)):\n",
    "#                     outputs[i] = outputs[i] / max_factor[i]\n",
    "        \n",
    "#             print(inputs.shape)\n",
    "#             print(outputs.shape)\n",
    "        \n",
    "            # Adding curvature as features\n",
    "            if self.transform:\n",
    "#                 print(inputs.shape)\n",
    "#                 print(outputs.shape)\n",
    "                inputs = np.array([transform_data(inputs, i) for i in range(len(inputs))])\n",
    "#                 print(inputs.shape)\n",
    "\n",
    "        self.start_pos = start_pos\n",
    "        self.rotate_matrix = rotate_factor # np.linalg.inv(rot[i].T) to reverse back\n",
    "        \n",
    "        if self.normalized:\n",
    "            self.n_max = max_factor\n",
    "\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ee1e4",
   "metadata": {},
   "source": [
    "###  Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da5b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multi-head self-attention module'''\n",
    "    def __init__(self, D, H):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.H = H # number of heads\n",
    "        self.D = D # dimension\n",
    "        \n",
    "        self.wq = nn.Linear(D, D*H)\n",
    "        self.wk = nn.Linear(D, D*H)\n",
    "        self.wv = nn.Linear(D, D*H)\n",
    "\n",
    "        self.dense = nn.Linear(D*H, D)\n",
    "\n",
    "    def concat_heads(self, x):\n",
    "        B, H, S, D = x.shape\n",
    "        x = x.permute((0, 2, 1, 3)).contiguous() \n",
    "        x = x.reshape((B, S, H*D))\n",
    "        return x\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, S, D_H = x.shape\n",
    "        x = x.reshape(B, S, self.H, self.D)\n",
    "        x = x.permute((0, 2, 1, 3))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.D)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            attention_scores += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n",
    "        scaled_attention = torch.matmul(attention_weights, v)\n",
    "        concat_attention = self.concat_heads(scaled_attention)\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd474fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encodings\n",
    "def get_angles(pos, i, D):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(D, position=60, dim=3, device=device):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(D)[np.newaxis, :],\n",
    "                            D)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    if dim == 3:\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    elif dim == 4:\n",
    "        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n",
    "    return torch.tensor(pos_encoding, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d86646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size, device=device):\n",
    "    mask = torch.ones((size, size), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a24212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n",
    "        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n",
    "        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n",
    "        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.mha = MultiHeadAttention(D, H)\n",
    "\n",
    "\n",
    "    def forward(self, x, look_ahead_mask):\n",
    "        \n",
    "        attn, attn_weights = self.mha(x, look_ahead_mask)\n",
    "        attn = self.dropout1(attn)\n",
    "        attn = self.layernorm1(attn + x)\n",
    "\n",
    "        mlp_act = torch.relu(self.mlp_hidden(attn))\n",
    "        mlp_act = self.mlp_out(mlp_act)\n",
    "        mlp_act = self.dropout2(mlp_act)\n",
    "        \n",
    "        output = self.layernorm2(mlp_act + attn)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcba1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Transformer Encoder\n",
    "    '''\n",
    "    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features,\n",
    "                 out_features, dropout_rate, batch_size, kernel_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.sqrt_D = torch.tensor(math.sqrt(D))\n",
    "        self.num_layers = num_layers\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(inp_features, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, D),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(50*D, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, out_features)\n",
    "        )\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(D)\n",
    "        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim, \n",
    "                                        dropout_rate=dropout_rate\n",
    "                                       ) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, S, D = x.shape\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        x *= self.sqrt_D\n",
    "        \n",
    "        x += self.pos_encoding[:, :S, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block = self.dec_layers[i](x=x,\n",
    "                                          look_ahead_mask=mask)\n",
    "            attention_weights['decoder_layer{}'.format(i + 1)] = block\n",
    "        \n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        return x, attention_weights\n",
    "    \n",
    "    def auto_regressor(self, x, mask, step):\n",
    "        \n",
    "        B, S, D = x.shape\n",
    "        new_inputs = torch.clone(x)\n",
    "        temp_pred, atn = self.forward(new_inputs, mask)\n",
    "        temp_pred = temp_pred.reshape(B, -1, 2)\n",
    "        new_inputs = torch.cat((new_inputs, temp_pred), 1)\n",
    "        \n",
    "        \n",
    "        for idx in range(step, 60, step):\n",
    "            train_inputs = new_inputs[:, idx:idx+50, :]\n",
    "            \n",
    "            starting_pos = torch.unsqueeze(train_inputs[:, 0, :], dim=1)\n",
    "            Q = torch.from_numpy(rotate(train_inputs.cpu().detach().numpy(), 0, 9, 30)).to(device)\n",
    "            trans_inputs = torch.matmul((train_inputs - starting_pos),\n",
    "                                        torch.transpose(Q, 1, 2))\n",
    "            \n",
    "            temp_pred, attention = self.forward(train_inputs, mask)\n",
    "            temp_pred = temp_pred.reshape(B, -1, 2)\n",
    "            temp_pred = (torch.matmul(temp_pred, Q) + starting_pos)\n",
    "            new_inputs = torch.cat((new_inputs, temp_pred), 1)\n",
    "            \n",
    "        return new_inputs[:, 50:].reshape(B, -1), attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bfac29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(test_loader, batch_sz, model):\n",
    "    '''\n",
    "    Remember to use test_dataset stats, NOT train_dataset\n",
    "    '''\n",
    "    count_row = 0\n",
    "    out = []\n",
    "\n",
    "    for X in test_loader:\n",
    "        if len(X) != batch_sz:\n",
    "            print(len(X))\n",
    "            to_fill = np.zeros([batch_sz-len(X), 50, 2])\n",
    "            X = torch.from_numpy(np.append(X, to_fill, axis=0))\n",
    "            \n",
    "#             a = test_dataset.rotate_matrix[-1].T\n",
    "#             temp = X[20]@np.linalg.inv(a) + test_dataset.start_pos[-1]\n",
    "#             plt.scatter(temp[:, 0], temp[:, 1], label='input')\n",
    "\n",
    "        X = X.to(device).float()\n",
    "    \n",
    "        S = X.shape[1]\n",
    "        mask = create_look_ahead_mask(S)\n",
    "\n",
    "        pred = model(X, mask)[0].reshape(batch_sz, -1, 2).cpu().detach().numpy()\n",
    "\n",
    "        for i in range(batch_sz):\n",
    "            if count_row >= len(test_dataset):\n",
    "                break\n",
    "\n",
    "            rotation =  test_dataset.rotate_matrix[count_row].T\n",
    "            pred[i] = pred[i] @ np.linalg.inv(rotation)\n",
    "            pred[i] = pred[i] + test_dataset.start_pos[count_row, : ]\n",
    "\n",
    "                \n",
    "            out.append(pred[i])\n",
    "            count_row += 1 \n",
    "\n",
    "\n",
    "    out = np.array(out).reshape(len(test_dataset), -1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4ca6a",
   "metadata": {},
   "source": [
    "### Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3357cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(city, model_path, n_epochs, learning_rate, factor, patience, device=device, batch_size=32):\n",
    "    # create data\n",
    "    dataset = ArgoverseDataset(city = city, split = 'train', transform=False, normalized=True)\n",
    "    train_sz = int(len(dataset) * 0.9)\n",
    "    val_sz = len(dataset) - train_sz\n",
    "    train_loader, val_loader = torch.utils.data.random_split(dataset, [train_sz, val_sz])\n",
    "    train_loader = DataLoader(train_loader, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "    val_loader = DataLoader(val_loader, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    # load model and utility\n",
    "    transformer = torch.load(model_path)\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate) \n",
    "    loss_function = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor,\n",
    "                                  patience=patience, verbose=True) \n",
    "    \n",
    "    # start training\n",
    "    avg_train_loss, avg_val_loss = [], []\n",
    "    train_time, elapsed_time = [], []\n",
    "    best_val_score = float('inf')\n",
    "    \n",
    "    for epoch in tqdm(list(range(n_epochs))):\n",
    "        print(f'Epoch {epoch+101}')\n",
    "        print('Training & Validating ', end='')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_loss, val_loss = [], []\n",
    "        \n",
    "        # Training set\n",
    "        for batches, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(DEVICE).float()\n",
    "            y = y.to(DEVICE).float()\n",
    "            \n",
    "            # Track progress\n",
    "            if (batches + 1) % 20 == 0:\n",
    "                print('-', end='')\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            S = X.shape[1]\n",
    "            mask = create_look_ahead_mask(S)\n",
    "            out, _ = transformer(X, mask) # .auto_regressor(X, mask, step)\n",
    "            \n",
    "#             print(out.shape)\n",
    "#             print(y.reshape(batch_size, -1).shape)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_function(out, y.reshape(batch_size, -1)) # y.reshape(batch_size, -1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        print()\n",
    "        avg_train = np.mean(train_loss)\n",
    "        avg_train_loss.append(avg_train)\n",
    "        \n",
    "        # End the time\n",
    "        end_train_time = time.time()\n",
    "        train_time.append(end_train_time - start_time)\n",
    "        \n",
    "        # Evaluate on val set\n",
    "        with torch.no_grad():\n",
    "            for batches, (X, y) in enumerate(val_loader):\n",
    "                X = X.to(DEVICE).float()\n",
    "                y = y.to(DEVICE).float()\n",
    "\n",
    "                S = X.shape[1]\n",
    "                mask = create_look_ahead_mask(S)\n",
    "                out, _ = transformer(X, mask) # .auto_regressor(X, mask, step)\n",
    "                loss = loss_function(out, y.reshape(batch_size, -1)) # y.reshape(batch_size, -1)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "            avg_val = np.mean(val_loss)\n",
    "            avg_val_loss.append(avg_val)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time.append(end_time - start_time)\n",
    "\n",
    "        print(f'- Training Loss: {avg_train}\\n- Validation Loss: {avg_val}')\n",
    "        print(f'- Train Time: {sum(train_time)}\\n- Elapsed Time: {sum(elapsed_time)}\\n')\n",
    "        \n",
    "        scheduler.step(avg_val)\n",
    "        \n",
    "        # save better model\n",
    "        if avg_val < best_val_score:\n",
    "            best_val_score = avg_val\n",
    "            torch.save(transformer, f'fine_model_{city}.pt')\n",
    "        \n",
    "    return transformer, (avg_train_loss, avg_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd1024",
   "metadata": {},
   "source": [
    "### City: Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81a5db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f461dbd34464a1abc78e5c72e7016c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.270596969620255\n",
      "- Validation Loss: 14.749081401682611\n",
      "- Train Time: 21.919984340667725\n",
      "- Elapsed Time: 22.444457530975342\n",
      "\n",
      "Epoch 102\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.226474562558261\n",
      "- Validation Loss: 14.652001633572935\n",
      "- Train Time: 44.263585805892944\n",
      "- Elapsed Time: 45.34610414505005\n",
      "\n",
      "Epoch 103\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.193496205195908\n",
      "- Validation Loss: 14.68318531050611\n",
      "- Train Time: 68.13679528236389\n",
      "- Elapsed Time: 69.73237752914429\n",
      "\n",
      "Epoch 104\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.174199531492123\n",
      "- Validation Loss: 14.653462335244933\n",
      "- Train Time: 91.01125025749207\n",
      "- Elapsed Time: 93.08648133277893\n",
      "\n",
      "Epoch     4: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 105\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.116212559928579\n",
      "- Validation Loss: 14.656590276689672\n",
      "- Train Time: 111.73426270484924\n",
      "- Elapsed Time: 114.31440615653992\n",
      "\n",
      "Epoch 106\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.096213922421795\n",
      "- Validation Loss: 14.688914163788752\n",
      "- Train Time: 134.17965722084045\n",
      "- Elapsed Time: 137.25158286094666\n",
      "\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 107\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.072841648030872\n",
      "- Validation Loss: 14.637800149063565\n",
      "- Train Time: 155.33605241775513\n",
      "- Elapsed Time: 158.86697244644165\n",
      "\n",
      "Epoch 108\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.058329378868923\n",
      "- Validation Loss: 14.647070923847938\n",
      "- Train Time: 176.03908896446228\n",
      "- Elapsed Time: 180.0515775680542\n",
      "\n",
      "Epoch 109\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.054430596690533\n",
      "- Validation Loss: 14.648882784060579\n",
      "- Train Time: 197.01979994773865\n",
      "- Elapsed Time: 201.55174326896667\n",
      "\n",
      "Epoch     9: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 110\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.043778015956406\n",
      "- Validation Loss: 14.640530970559192\n",
      "- Train Time: 219.760023355484\n",
      "- Elapsed Time: 224.84332728385925\n",
      "\n",
      "Epoch 111\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.041724108861498\n",
      "- Validation Loss: 14.63705335446258\n",
      "- Train Time: 241.6372265815735\n",
      "- Elapsed Time: 247.29923820495605\n",
      "\n",
      "Epoch    11: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch 112\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.030333485879188\n",
      "- Validation Loss: 14.632643845543933\n",
      "- Train Time: 263.71615839004517\n",
      "- Elapsed Time: 269.89942479133606\n",
      "\n",
      "Epoch 113\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.034359644464224\n",
      "- Validation Loss: 14.631275924284067\n",
      "- Train Time: 284.4859290122986\n",
      "- Elapsed Time: 291.1895704269409\n",
      "\n",
      "Epoch 114\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.033128544909895\n",
      "- Validation Loss: 14.634108372588656\n",
      "- Train Time: 305.4372253417969\n",
      "- Elapsed Time: 312.66025400161743\n",
      "\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-07.\n",
      "Epoch 115\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.020105089234912\n",
      "- Validation Loss: 14.631925995670148\n",
      "- Train Time: 326.35448241233826\n",
      "- Elapsed Time: 334.1834890842438\n",
      "\n",
      "Epoch 116\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.024758025240306\n",
      "- Validation Loss: 14.632642084093236\n",
      "- Train Time: 349.4642562866211\n",
      "- Elapsed Time: 357.83526253700256\n",
      "\n",
      "Epoch    16: reducing learning rate of group 0 to 3.1250e-07.\n",
      "Epoch 117\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.025462714502634\n",
      "- Validation Loss: 14.632755649623586\n",
      "- Train Time: 370.57986855506897\n",
      "- Elapsed Time: 379.50694465637207\n",
      "\n",
      "Epoch 118\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.023419970126191\n",
      "- Validation Loss: 14.632606129148114\n",
      "- Train Time: 393.87496161460876\n",
      "- Elapsed Time: 403.56305027008057\n",
      "\n",
      "Epoch    18: reducing learning rate of group 0 to 1.5625e-07.\n",
      "Epoch 119\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.022721846241597\n",
      "- Validation Loss: 14.632621903917682\n",
      "- Train Time: 417.7471079826355\n",
      "- Elapsed Time: 428.16176319122314\n",
      "\n",
      "Epoch 120\n",
      "Training & Validating ------------------------------------------------------------\n",
      "- Training Loss: 15.020845084150961\n",
      "- Validation Loss: 14.632531013061751\n",
      "- Train Time: 443.1649901866913\n",
      "- Elapsed Time: 454.37500500679016\n",
      "\n",
      "Epoch    20: reducing learning rate of group 0 to 7.8125e-08.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "city = 'austin'\n",
    "model_path = 'best_total.pt'\n",
    "n_epochs = 20 # [50, 100]\n",
    "learning_rate = 0.00002 # [0.001, 0.01] 0.002\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 1\n",
    "\n",
    "austin_net, austin_losses = fine_tune(city, model_path, n_epochs, learning_rate, factor, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97270f",
   "metadata": {},
   "source": [
    "### City: Miami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b826271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "city = 'miami'\n",
    "model_path = 'best_total.pt'\n",
    "n_epochs = 20 # [50, 100]\n",
    "learning_rate = 0.00002 # [0.001, 0.01] 0.002\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 1\n",
    "\n",
    "miami_net, miami_losses = fine_tune(city, model_path, n_epochs, learning_rate, factor, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e7c2b",
   "metadata": {},
   "source": [
    "### City: Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "city = 'pittsburgh'\n",
    "model_path = 'best_total.pt'\n",
    "n_epochs = 20 # [50, 100]\n",
    "learning_rate = 0.00002 # [0.001, 0.01] 0.002\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 1\n",
    "\n",
    "pitts_net, pitts_losses = fine_tune(city, model_path, n_epochs, learning_rate, factor, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e5c53",
   "metadata": {},
   "source": [
    "### City: Dearborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "city = 'dearborn'\n",
    "model_path = 'best_total.pt'\n",
    "n_epochs = 20 # [50, 100]\n",
    "learning_rate = 0.00002 # [0.001, 0.01] 0.002\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 1\n",
    "\n",
    "dearborn_net, dearborn_losses = fine_tune(city, model_path, n_epochs, learning_rate, factor, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cd5e8",
   "metadata": {},
   "source": [
    "### City: Washington D.C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "city = 'washington-dc'\n",
    "model_path = 'best_total.pt'\n",
    "n_epochs = 20 # [50, 100]\n",
    "learning_rate = 0.00002 # [0.001, 0.01] 0.002\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 1\n",
    "\n",
    "wash_net, wash_losses = fine_tune(city, model_path, n_epochs, learning_rate, factor, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4414da2",
   "metadata": {},
   "source": [
    "### City: Palo Alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac905f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ef95cd427c4ead8d5ec8394a09b5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 15.139370829132261\n",
      "- Validation Loss: 14.980401090673498\n",
      "- Train Time: 5.84048867225647\n",
      "- Elapsed Time: 5.968170166015625\n",
      "\n",
      "Epoch 102\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.961215251277391\n",
      "- Validation Loss: 14.933151219342205\n",
      "- Train Time: 12.256375074386597\n",
      "- Elapsed Time: 12.510830879211426\n",
      "\n",
      "Epoch 103\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.877133031273454\n",
      "- Validation Loss: 14.955894109365103\n",
      "- Train Time: 17.986387491226196\n",
      "- Elapsed Time: 18.37568211555481\n",
      "\n",
      "Epoch 104\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.809772420707725\n",
      "- Validation Loss: 14.920741545187461\n",
      "- Train Time: 23.75948929786682\n",
      "- Elapsed Time: 24.282828092575073\n",
      "\n",
      "Epoch 105\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.754254384988847\n",
      "- Validation Loss: 14.936514622456318\n",
      "- Train Time: 29.39859366416931\n",
      "- Elapsed Time: 30.052507638931274\n",
      "\n",
      "Epoch 106\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.720417252633975\n",
      "- Validation Loss: 14.902960674182788\n",
      "- Train Time: 34.95425486564636\n",
      "- Elapsed Time: 35.73957324028015\n",
      "\n",
      "Epoch 107\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.677288840715539\n",
      "- Validation Loss: 14.938399907704946\n",
      "- Train Time: 41.382004261016846\n",
      "- Elapsed Time: 42.293445110321045\n",
      "\n",
      "Epoch 108\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.629392967733502\n",
      "- Validation Loss: 14.922237370465252\n",
      "- Train Time: 46.703275203704834\n",
      "- Elapsed Time: 47.744911432266235\n",
      "\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 109\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.586594416055199\n",
      "- Validation Loss: 14.901132222768423\n",
      "- Train Time: 53.03057241439819\n",
      "- Elapsed Time: 54.208834409713745\n",
      "\n",
      "Epoch 110\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.543566715115963\n",
      "- Validation Loss: 14.894825729163918\n",
      "- Train Time: 58.815667390823364\n",
      "- Elapsed Time: 60.1235249042511\n",
      "\n",
      "Epoch 111\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.537483806779901\n",
      "- Validation Loss: 14.920356183438688\n",
      "- Train Time: 64.76874423027039\n",
      "- Elapsed Time: 66.20826816558838\n",
      "\n",
      "Epoch 112\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.533158764994816\n",
      "- Validation Loss: 14.93988882528769\n",
      "- Train Time: 70.27624011039734\n",
      "- Elapsed Time: 71.84677314758301\n",
      "\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 113\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.489281869429863\n",
      "- Validation Loss: 14.899495356791729\n",
      "- Train Time: 75.85115599632263\n",
      "- Elapsed Time: 77.56078481674194\n",
      "\n",
      "Epoch 114\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.480964094665708\n",
      "- Validation Loss: 14.9017253050933\n",
      "- Train Time: 81.24556946754456\n",
      "- Elapsed Time: 83.08374285697937\n",
      "\n",
      "Epoch    14: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 115\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.465364283553217\n",
      "- Validation Loss: 14.893470609510267\n",
      "- Train Time: 88.1732828617096\n",
      "- Elapsed Time: 90.13966083526611\n",
      "\n",
      "Epoch 116\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.468939130313318\n",
      "- Validation Loss: 14.893982887268066\n",
      "- Train Time: 94.81505680084229\n",
      "- Elapsed Time: 96.9132616519928\n",
      "\n",
      "Epoch    16: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch 117\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.45106794430877\n",
      "- Validation Loss: 14.894236641961175\n",
      "- Train Time: 100.84993600845337\n",
      "- Elapsed Time: 103.10699272155762\n",
      "\n",
      "Epoch 118\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.458222063783369\n",
      "- Validation Loss: 14.896561854594463\n",
      "- Train Time: 106.43305897712708\n",
      "- Elapsed Time: 108.8176748752594\n",
      "\n",
      "Epoch    18: reducing learning rate of group 0 to 6.2500e-07.\n",
      "Epoch 119\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.458406449779204\n",
      "- Validation Loss: 14.895115671931086\n",
      "- Train Time: 111.84336066246033\n",
      "- Elapsed Time: 114.35977482795715\n",
      "\n",
      "Epoch 120\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 14.449509644720603\n",
      "- Validation Loss: 14.895057162723026\n",
      "- Train Time: 117.87499666213989\n",
      "- Elapsed Time: 120.53378057479858\n",
      "\n",
      "Epoch    20: reducing learning rate of group 0 to 3.1250e-07.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "city = 'palo-alto'\n",
    "model_path = 'best_total.pt'\n",
    "n_epochs = 20 # [50, 100]\n",
    "learning_rate = 0.00002 # [0.001, 0.01] 0.002\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 1\n",
    "\n",
    "palo_net, palo_losses = fine_tune(city, model_path, n_epochs, learning_rate, factor, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59658136",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad45c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6325, 120)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'austin' \n",
    "\n",
    "test_dataset = ArgoverseDataset(city = city, split = 'test', transform=False, normalized=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "austin_array = make_pred(test_loader, 32, austin_net)\n",
    "austin_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68ca6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_df = pd.DataFrame(austin_array)\n",
    "austin_df.to_csv('austin_tuned_0529.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfe1122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7971, 120)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'miami' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[1]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "miami_array = make_pred(test_loader, 32, total_net)\n",
    "miami_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e14bc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6361, 120)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'pittsburgh' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[2]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "pitts_array = make_pred(test_loader, 32, total_net)\n",
    "pitts_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a392f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3671, 120)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'dearborn' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[3]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "dearborn_array = make_pred(test_loader, 32, total_net)\n",
    "dearborn_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a95dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3829, 120)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'washington-dc' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[4]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "wash_array = make_pred(test_loader, 32, total_net)\n",
    "wash_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c604249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1686, 120)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'palo-alto' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[5]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "palo_array = make_pred(test_loader, 32, total_net)\n",
    "palo_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd7b70",
   "metadata": {},
   "source": [
    "### Write File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad715572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6325\n",
      "7971\n",
      "6361\n",
      "3671\n",
      "3829\n",
      "1686\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "cols = [['ID'] + ['v{}'.format(i) for i in range(120)]]\n",
    "\n",
    "with open('output.csv', 'w+') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    mywriter.writerows(cols)\n",
    "\n",
    "with open('output.csv', 'a') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(austin_array)):\n",
    "        temp = [np.append(['{}_austin'.format(i)], austin_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(miami_array)):\n",
    "        temp = [np.append(['{}_miami'.format(i)], miami_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(pitts_array)):\n",
    "        temp = [np.append(['{}_pittsburgh'.format(i)], pitts_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(dearborn_array)):\n",
    "        temp = [np.append(['{}_dearborn'.format(i)], dearborn_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(wash_array)):\n",
    "        temp = [np.append(['{}_washington-dc'.format(i)], wash_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(palo_array)):\n",
    "        temp = [np.append(['{}_palo-alto'.format(i)], palo_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
