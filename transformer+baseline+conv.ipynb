{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3d7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy as np\n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "\"\"\"\n",
    "ROOT_PATH = \"./\"\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e3292",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6db7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix(inputs, device=DEVICE): \n",
    "    vectors = inputs[:, 1, :]\n",
    "    matrices = torch.zeros((vectors.shape[0], 2, 2)).to(device).float()\n",
    "    for i in range(vectors.shape[0]):\n",
    "        matrices[i] = rotate_matrix(vectors[i]).to(device)\n",
    "    return matrices\n",
    "\n",
    "def rotate_matrix(v, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Rotates the vector into the (1, 1) direction\n",
    "    \"\"\"    \n",
    "    a, b = v\n",
    "    norm = torch.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        Q1 = torch.eye(2).to(device)\n",
    "    else:\n",
    "        Q1 = torch.tensor([[a, b], [-b, a]]).to(device) / torch.linalg.norm(v)\n",
    "    Q2 = torch.tensor([[1, -1], [1, 1]]).to(device) / torch.sqrt(torch.tensor(2).to(device))\n",
    "    \n",
    "    return Q2.float() @ Q1.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfd79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(A, Q, T):\n",
    "#     return torch.matmul((A - T.unsqueeze(1)), Q.T)\n",
    "\n",
    "def revert(A, Q, T):\n",
    "    return torch.matmul(A, Q) + T.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1cd8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=False, normalize=False):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.split = split\n",
    "        self.inputs, self.outputs = self.get_city_trajectories(city=city, split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'train':       \n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "            return data     \n",
    "        return self.inputs[idx]\n",
    "    \n",
    "    def get_city_trajectories(self, city=\"palo-alto\", split=\"train\", device=DEVICE):\n",
    "        assert city in cities and split in splits\n",
    "\n",
    "        # get input\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        inputs = torch.tensor(inputs).to(device).float()\n",
    "\n",
    "        # store input starting positions and rotation matrix\n",
    "        self.T0 = inputs[:, 0, :]\n",
    "\n",
    "        # normalize inputs\n",
    "        if self.normalize:     \n",
    "            # translation\n",
    "            inputs = inputs - self.T0.unsqueeze(1)\n",
    "            \n",
    "            # rotation\n",
    "            self.Q0 = get_rotation_matrix(inputs)\n",
    "            inputs = torch.matmul(inputs, self.Q0.transpose(1, 2))\n",
    "\n",
    "        # get outputs\n",
    "        outputs = None\n",
    "        if split == \"train\":\n",
    "            # get outputs\n",
    "            f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "            outputs = pickle.load(open(f_out, \"rb\"))\n",
    "            outputs = torch.tensor(outputs).to(device).float()\n",
    "            \n",
    "            # normalize outputs\n",
    "            if self.normalize:      \n",
    "                # translation\n",
    "                outputs = outputs - self.T0.unsqueeze(1)\n",
    "                    \n",
    "                # rotation\n",
    "                outputs = torch.matmul(outputs, self.Q0.transpose(1, 2))\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30b1b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11993"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train' #'test' #'train'\n",
    "train_dataset = ArgoverseDataset(city = city, split = split, transform=False, normalize=True)\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7877ed6",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97632d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sz = 4  # batch size \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz, drop_last=True)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfef58",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea7e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    inp = inp.cpu().detach().numpy()\n",
    "    out = out.cpu().detach().numpy()\n",
    "    batch_sz, agent_sz = inp.shape[0], inp.shape[1]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace=.2, wspace=.5)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(4):\n",
    "        axs[i].xaxis.set_ticks([inp[i,0,0],\n",
    "                                   inp[i,-1,0], out[i,-1,0]])\n",
    "        axs[i].yaxis.set_ticks([inp[i,0,1],\n",
    "                                   inp[i,-1,1], out[i,-1,1]])\n",
    "\n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b1a2362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i_batch, data in enumerate(train_loader):\n",
    "    show_sample_batch(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ee1e4",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d88f6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAutoReg(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        model_dim,\n",
    "        conv_depth=16,\n",
    "        kernel_size=4,\n",
    "        conv_padding=3,\n",
    "        n_heads=8, \n",
    "        linear_dim=256, \n",
    "        dropout_rate=0.1, \n",
    "        encoder_layers=1,\n",
    "        input_dim=2,\n",
    "        regress_steps=4\n",
    "    ):\n",
    "        super(TransAutoReg, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.model_dim = model_dim\n",
    "        self.conv_depth = conv_depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_padding = conv_padding\n",
    "        self.n_heads = n_heads\n",
    "        self.linear_dim = linear_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.input_dim = input_dim\n",
    "        self.regress_steps = regress_steps\n",
    "        self.input_length = 50\n",
    "        self.output_length = 10\n",
    "        \n",
    "        # compute convolution output size\n",
    "        size_1 = self.input_length + 2 * conv_padding - kernel_size + 1\n",
    "        size_2 = size_1 + 2 * conv_padding - kernel_size + 1\n",
    "        self.conv_outsize = (size_2 - kernel_size) // 2 + 1\n",
    "        \n",
    "        # input convolution\n",
    "        self.input_convolution = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, conv_depth, kernel_size=kernel_size, padding=conv_padding),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(conv_depth, conv_depth, kernel_size=kernel_size, padding=conv_padding),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
    "        )  \n",
    "        self.input_projection = nn.Linear(conv_depth, model_dim)\n",
    "        \n",
    "        # encoder\n",
    "        encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=linear_dim, \n",
    "            dropout=dropout_rate, \n",
    "            batch_first=True,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder, num_layers=encoder_layers, norm=None)\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.conv_outsize * model_dim, linear_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(linear_dim, linear_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(linear_dim, self.output_length * input_dim)\n",
    "        )\n",
    "    \n",
    "    def get_angles(self, pos, i, D):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, D, position=110, dim=3, device=DEVICE):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                np.arange(D)[np.newaxis, :],\n",
    "                                D)\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        if dim == 3:\n",
    "            pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        elif dim == 4:\n",
    "            pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n",
    "        return torch.tensor(pos_encoding, device=device)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_pos = self.input_convolution(inputs.transpose(1, 2))\n",
    "        input_pos = self.input_projection(input_pos.transpose(1, 2))\n",
    "        input_pos += self.positional_encoding(self.model_dim)[:, :self.conv_outsize, :]\n",
    "        memory = self.encoder(input_pos).reshape(self.batch_size, -1)\n",
    "        out = self.decoder(memory)\n",
    "        return out.reshape(self.batch_size, self.output_length, self.input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57610ecc",
   "metadata": {},
   "source": [
    "## Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d033cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    batch_size, \n",
    "    city, \n",
    "    model_dim,\n",
    "    conv_depth,\n",
    "    kernel_size,\n",
    "    conv_padding,\n",
    "    n_heads,\n",
    "    linear_dim,\n",
    "    dropout_rate,\n",
    "    encoder_layers,\n",
    "    regress_steps,\n",
    "    num_iters,\n",
    "    learning_rate,\n",
    "    factor,\n",
    "    patience,\n",
    "    device=DEVICE\n",
    "):    \n",
    "    # Create the training/validation set\n",
    "    train_dataset = ArgoverseDataset(city=city, split='train', transform=False, normalize=True)\n",
    "    train_sz = int(len(train_dataset) * 0.9)\n",
    "    val_sz = len(train_dataset) - train_sz\n",
    "    train_loader, val_loader = torch.utils.data.random_split(train_dataset, [train_sz, val_sz])\n",
    "    train_loader = DataLoader(train_loader, batch_size=batch_size, drop_last=True)\n",
    "    val_loader = DataLoader(val_loader, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    # Initialize the model/optimizer/loss function\n",
    "    model = TransAutoReg(\n",
    "        batch_size=batch_size,\n",
    "        model_dim=model_dim,\n",
    "        conv_depth=conv_depth,\n",
    "        kernel_size=kernel_size,\n",
    "        conv_padding=conv_padding,\n",
    "        n_heads=n_heads, \n",
    "        linear_dim=linear_dim, \n",
    "        dropout_rate=dropout_rate, \n",
    "        encoder_layers=encoder_layers,\n",
    "        regress_steps=regress_steps\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    loss_function = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, verbose=True) \n",
    "    \n",
    "    # Print out how many parameters to train\n",
    "    param_sizes = [p.numel() for p in model.parameters()]\n",
    "    print(f\"number of weight/biases matrices: {len(param_sizes)} \"\n",
    "          f\"for a total of {np.sum(param_sizes)} parameters \")\n",
    "    \n",
    "    avg_train_loss, avg_val_loss = [], []\n",
    "    best_val_score = float('inf')\n",
    "    \n",
    "    # Start training\n",
    "    for epoch in tqdm(list(range(num_iters))):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print('Training & Validating ', end='')\n",
    "        \n",
    "        train_loss, val_loss = [], []\n",
    "        \n",
    "        # Training set\n",
    "        for batches, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            # Track progress\n",
    "            if (batches + 1) % 20 == 0:\n",
    "                print('-', end='')\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_function(out, y[:, :10, :])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "        print()\n",
    "        avg_train = np.mean(train_loss)\n",
    "        avg_train_loss.append(avg_train)\n",
    "        \n",
    "\n",
    "        # Evaluate on val set\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(device).float()\n",
    "                y = y.to(device).float()\n",
    "                \n",
    "                out = predict(model, X)\n",
    "                loss = loss_function(out, y)\n",
    "                val_loss.append(loss.item())\n",
    "                \n",
    "            avg_val = np.mean(val_loss)\n",
    "            avg_val_loss.append(avg_val)\n",
    "\n",
    "        print(f'- Training Loss: {avg_train}')\n",
    "        print(f'- Validation Loss: {avg_val}')\n",
    "        print()\n",
    "        \n",
    "        scheduler.step(avg_val)\n",
    "        \n",
    "        # save better model\n",
    "        if avg_val < best_val_score:\n",
    "            best_val_score = avg_val\n",
    "            torch.save(model, f'TF_Auto_Conv_{city}_0527.pt')\n",
    "        \n",
    "    return model, (avg_train_loss, avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5181c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, inputs):\n",
    "    with torch.no_grad():\n",
    "        total_steps = 60 // model.output_length\n",
    "\n",
    "        # predict first roll\n",
    "        step_pred = model(inputs)\n",
    "#         print(inputs.shape, step_pred.shape)\n",
    "        inputs = torch.cat((\n",
    "            inputs,\n",
    "            step_pred\n",
    "        ), dim=1)\n",
    "\n",
    "        for step in range(1, total_steps):\n",
    "            # take chunk\n",
    "            cur_inputs = inputs[:, step * model.output_length:, :].clone()\n",
    "\n",
    "            # normalize\n",
    "            T_t = cur_inputs[:, 0, :].clone()\n",
    "            cur_inputs = cur_inputs - T_t.unsqueeze(1)\n",
    "\n",
    "            Q_t = get_rotation_matrix(cur_inputs)\n",
    "            cur_inputs = torch.matmul(cur_inputs, Q_t.transpose(1, 2))\n",
    "\n",
    "            # autoregress\n",
    "            step_pred = model(cur_inputs)\n",
    "\n",
    "            # revert to original scale\n",
    "            step_pred = torch.matmul(step_pred, Q_t) + T_t.unsqueeze(1)\n",
    "            \n",
    "            # add new predictions\n",
    "            inputs = torch.cat((\n",
    "                inputs,\n",
    "                step_pred\n",
    "            ), dim=1)\n",
    "\n",
    "        return inputs[:, model.input_length:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f29b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(start_from, city, split, losses):\n",
    "\n",
    "    avg_train_loss, avg_val_loss = losses\n",
    "    plt.plot(np.sqrt(avg_train_loss[start_from:]), label='train_loss')\n",
    "    plt.plot(np.sqrt(avg_val_loss[start_from:]), label='validation_loss')\n",
    "    plt.title(f'{city} RMSE {split} Loss vs. Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59a82219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_results(city, split, batch_size, model, idx, device=DEVICE):\n",
    "    train_dataset = ArgoverseDataset(city = city, split = split, transform=False, normalize=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "\n",
    "            out = model(X).reshape(batch_size, -1, 2)\n",
    "            break\n",
    "    \n",
    "    X = X.cpu()\n",
    "    y = y.cpu()\n",
    "    out = out.cpu()\n",
    "    \n",
    "    Q = train_dataset.Q0[idx].cpu()\n",
    "    T = train_dataset.T0[idx].cpu()\n",
    "    X = X[idx] @ Q + T\n",
    "    y = y[idx] @ Q + T\n",
    "    out = out[idx] @ Q + T\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], label='seed')\n",
    "    plt.scatter(y[:, 0], y[:, 1], label='ground truth')\n",
    "    plt.scatter(out[:, 0], out[:, 1], label='prediction')\n",
    "    plt.title(f'Random Sample From {city}_{split} Projectile Visualization')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "66b3efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_test(city, split, batch_sz, model, idx, norm_viz=True):\n",
    "    '''\n",
    "    This is the last batch, which is usually not complete and need to fill with 0s.\n",
    "    Check if I convert the prediction back correctly or not\n",
    "    '''\n",
    "    \n",
    "    # Create the dataset for testing\n",
    "    test_dataset = ArgoverseDataset(city = city, split = split,\n",
    "                                    transform=False, normalized=True)\n",
    "        \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_sz)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X in test_loader:\n",
    "            if len(X) == batch_sz:\n",
    "                continue\n",
    "            print(len(X))\n",
    "            to_fill = np.zeros([batch_sz-len(X), 50, 2])\n",
    "            X = torch.from_numpy(np.append(X, to_fill, axis=0)).float()\n",
    "            X = X.to(device).float()\n",
    "            S = X.shape[1]\n",
    "            mask = create_look_ahead_mask(S)\n",
    "\n",
    "            output = model(X, mask)[0].reshape(batch_size, -1, 2)\n",
    "\n",
    "            X = X.cpu()\n",
    "            output = output.cpu()\n",
    "\n",
    "            if norm_viz:\n",
    "                plt.scatter(X[idx, :, 0], X[idx, :, 1], label='input')\n",
    "                plt.scatter(output[idx, :, 0], output[idx, :, 1], label='pred')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            else:\n",
    "                rotation =  test_dataset.rotate_matrix[-1].T\n",
    "                X[idx] = X[idx] @ np.linalg.inv(rotation)\n",
    "                X[idx] = X[idx] + test_dataset.start_pos[-1, : ]\n",
    "                output[idx] = output[idx] @ np.linalg.inv(rotation)\n",
    "                output[idx] = output[idx] + test_dataset.start_pos[-1, : ]\n",
    "\n",
    "                plt.scatter(X[idx, :, 0], X[idx, :, 1], label='input')\n",
    "                plt.scatter(output[idx, :, 0], output[idx, :, 1], label='pred')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6b019b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(test_loader, batch_sz, model):\n",
    "    '''\n",
    "    Remember to use test_dataset stats, NOT train_dataset\n",
    "    '''\n",
    "    count_row = 0\n",
    "    out = []\n",
    "\n",
    "    for X in test_loader:\n",
    "        if len(X) != batch_sz:\n",
    "            print(len(X))\n",
    "            to_fill = np.zeros([batch_sz-len(X), 50, 2])\n",
    "            X = torch.from_numpy(np.append(X, to_fill, axis=0))\n",
    "            \n",
    "#             a = test_dataset.rotate_matrix[-1].T\n",
    "#             temp = X[20]@np.linalg.inv(a) + test_dataset.start_pos[-1]\n",
    "#             plt.scatter(temp[:, 0], temp[:, 1], label='input')\n",
    "\n",
    "        X = X.to(device).float()\n",
    "    \n",
    "        S = X.shape[1]\n",
    "        mask = create_look_ahead_mask(S)\n",
    "\n",
    "        pred = model(X, mask)[0].reshape(batch_size, -1, 2).cpu().detach().numpy()\n",
    "\n",
    "        for i in range(batch_sz):\n",
    "            if count_row >= len(test_dataset):\n",
    "                break\n",
    "                \n",
    "#             if count_row == (len(test_dataset) - 1):\n",
    "#                 plt.scatter(X[i, :, 0], X[i, :, 1], label='input')\n",
    "#                 plt.scatter(pred[i, :, 0], pred[i, :, 1], label='pred')\n",
    "#                 plt.legend()\n",
    "#                 plt.show()\n",
    "\n",
    "            rotation =  test_dataset.rotate_matrix[count_row].T\n",
    "            pred[i] = pred[i] @ np.linalg.inv(rotation)\n",
    "            pred[i] = pred[i] + test_dataset.start_pos[count_row, : ]\n",
    "\n",
    "    #         print(pred[i, 0, :])\n",
    "                \n",
    "            out.append(pred[i])\n",
    "            count_row += 1 \n",
    "            \n",
    "    #         print(count_row)\n",
    "    #         print(pred[0, :5, :])\n",
    "\n",
    "    out = np.array(out).reshape(len(test_dataset), -1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100eb9b",
   "metadata": {},
   "source": [
    "### palo-alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2bc3960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of weight/biases matrices: 60 for a total of 868564 parameters \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2219cf28947942bbae4ee38259718183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 46.6364538648362\n",
      "- Validation Loss: 111.65736100480363\n",
      "\n",
      "Epoch 2\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 5.944466229008638\n",
      "- Validation Loss: 216.59598128860063\n",
      "\n",
      "Epoch 3\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 3.232091017222192\n",
      "- Validation Loss: 141.08730274922138\n",
      "\n",
      "Epoch 4\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 5.285408096249804\n",
      "- Validation Loss: 91.66031842618375\n",
      "\n",
      "Epoch 5\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 3.1711898610570666\n",
      "- Validation Loss: 146.3517608642578\n",
      "\n",
      "Epoch 6\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 3.0199239533449846\n",
      "- Validation Loss: 182.2935182726061\n",
      "\n",
      "Epoch 7\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 2.8647193342712582\n",
      "- Validation Loss: 163.8831123145851\n",
      "\n",
      "Epoch 8\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 2.740385043426508\n",
      "- Validation Loss: 106.99883641423406\n",
      "\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 9\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.8370211153161278\n",
      "- Validation Loss: 86.10973935513883\n",
      "\n",
      "Epoch 10\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.8001038969801159\n",
      "- Validation Loss: 63.55011723492596\n",
      "\n",
      "Epoch 11\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.7647530376380558\n",
      "- Validation Loss: 53.467660646180846\n",
      "\n",
      "Epoch 12\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.9350759807078703\n",
      "- Validation Loss: 82.34092135042758\n",
      "\n",
      "Epoch 13\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.8445184182165638\n",
      "- Validation Loss: 51.78349000054437\n",
      "\n",
      "Epoch 14\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.9297481002368984\n",
      "- Validation Loss: 55.936500549316406\n",
      "\n",
      "Epoch 15\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 1.2100003018577303\n",
      "- Validation Loss: 60.20906974173881\n",
      "\n",
      "Epoch 16\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.9925540960684021\n",
      "- Validation Loss: 113.0702184728674\n",
      "\n",
      "Epoch 17\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 1.1613875848603177\n",
      "- Validation Loss: 60.6611798260663\n",
      "\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 18\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.5434012515608564\n",
      "- Validation Loss: 79.76569552034945\n",
      "\n",
      "Epoch 19\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.47866774909751\n",
      "- Validation Loss: 38.715338165695606\n",
      "\n",
      "Epoch 20\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.5477733291928776\n",
      "- Validation Loss: 42.20430626740327\n",
      "\n",
      "Epoch 21\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.5787869546816682\n",
      "- Validation Loss: 39.155457986367715\n",
      "\n",
      "Epoch 22\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.574149011283317\n",
      "- Validation Loss: 36.142101390941725\n",
      "\n",
      "Epoch 23\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.5271606571331222\n",
      "- Validation Loss: 52.12764647200301\n",
      "\n",
      "Epoch 24\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.6393626579898755\n",
      "- Validation Loss: 40.33173189936458\n",
      "\n",
      "Epoch 25\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.5370650399332584\n",
      "- Validation Loss: 42.55362655021049\n",
      "\n",
      "Epoch 26\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.5488025136181438\n",
      "- Validation Loss: 59.23989909404033\n",
      "\n",
      "Epoch    26: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 27\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.35205890673823453\n",
      "- Validation Loss: 35.77844578511006\n",
      "\n",
      "Epoch 28\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.31714722450037736\n",
      "- Validation Loss: 52.39986770217483\n",
      "\n",
      "Epoch 29\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.3353184480164808\n",
      "- Validation Loss: 91.4955375258987\n",
      "\n",
      "Epoch 30\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.34317195030808095\n",
      "- Validation Loss: 33.52030388084618\n",
      "\n",
      "Epoch 31\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.3322740317673287\n",
      "- Validation Loss: 46.98253368686985\n",
      "\n",
      "Epoch 32\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.354873787291149\n",
      "- Validation Loss: 41.764400482177734\n",
      "\n",
      "Epoch 33\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.34674965148067616\n",
      "- Validation Loss: 59.84767078708958\n",
      "\n",
      "Epoch 34\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.35202339446332404\n",
      "- Validation Loss: 56.09133282223263\n",
      "\n",
      "Epoch    34: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 35\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.264024325665805\n",
      "- Validation Loss: 41.028313971854544\n",
      "\n",
      "Epoch 36\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.25063133317126013\n",
      "- Validation Loss: 33.61658235498377\n",
      "\n",
      "Epoch 37\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.24041813061481412\n",
      "- Validation Loss: 31.084625269915605\n",
      "\n",
      "Epoch 38\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.2395662492036112\n",
      "- Validation Loss: 33.08627675030682\n",
      "\n",
      "Epoch 39\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.23655203128515437\n",
      "- Validation Loss: 31.31855126973745\n",
      "\n",
      "Epoch 40\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.2473314045002623\n",
      "- Validation Loss: 33.998868323661185\n",
      "\n",
      "Epoch 41\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.24336175912388708\n",
      "- Validation Loss: 28.94626823631493\n",
      "\n",
      "Epoch 42\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.23263788298435664\n",
      "- Validation Loss: 30.100138277620882\n",
      "\n",
      "Epoch 43\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.2332286543989394\n",
      "- Validation Loss: 31.54447081282332\n",
      "\n",
      "Epoch 44\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.2246612100820513\n",
      "- Validation Loss: 31.806665008132523\n",
      "\n",
      "Epoch 45\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.22987021982271877\n",
      "- Validation Loss: 31.515788980432458\n",
      "\n",
      "Epoch    45: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 46\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.2048478387185657\n",
      "- Validation Loss: 28.604073627575023\n",
      "\n",
      "Epoch 47\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.19564331218998227\n",
      "- Validation Loss: 28.555247719223434\n",
      "\n",
      "Epoch 48\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1933670720466874\n",
      "- Validation Loss: 29.501812187401026\n",
      "\n",
      "Epoch 49\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.18950822051802446\n",
      "- Validation Loss: 26.825118296855205\n",
      "\n",
      "Epoch 50\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1850274034529836\n",
      "- Validation Loss: 27.53919281830659\n",
      "\n",
      "Epoch 51\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.18932441759091811\n",
      "- Validation Loss: 29.166561642208613\n",
      "\n",
      "Epoch 52\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.19037573709420705\n",
      "- Validation Loss: 27.04940785588445\n",
      "\n",
      "Epoch 53\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.18813129633841955\n",
      "- Validation Loss: 32.578367542576146\n",
      "\n",
      "Epoch    53: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 54\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.17637359801934097\n",
      "- Validation Loss: 25.960075301093024\n",
      "\n",
      "Epoch 55\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.16896243878703796\n",
      "- Validation Loss: 26.017886084479255\n",
      "\n",
      "Epoch 56\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1735068899676071\n",
      "- Validation Loss: 25.803989822800094\n",
      "\n",
      "Epoch 57\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.16836583182127257\n",
      "- Validation Loss: 26.18893022794981\n",
      "\n",
      "Epoch 58\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.16590645466033005\n",
      "- Validation Loss: 25.802668519922204\n",
      "\n",
      "Epoch 59\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.16383506566595607\n",
      "- Validation Loss: 26.975550548450368\n",
      "\n",
      "Epoch 60\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1686121562180781\n",
      "- Validation Loss: 26.12655765945847\n",
      "\n",
      "Epoch    60: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 61\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.15757640590476707\n",
      "- Validation Loss: 26.18399852030986\n",
      "\n",
      "Epoch 62\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1586437797511013\n",
      "- Validation Loss: 26.55235612714613\n",
      "\n",
      "Epoch 63\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.15628189046366986\n",
      "- Validation Loss: 25.462988776129645\n",
      "\n",
      "Epoch 64\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.15553001200542604\n",
      "- Validation Loss: 25.28417172303071\n",
      "\n",
      "Epoch 65\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.15750699631803466\n",
      "- Validation Loss: 25.847180057216335\n",
      "\n",
      "Epoch 66\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.157354822144664\n",
      "- Validation Loss: 25.55463319211393\n",
      "\n",
      "Epoch 67\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1605119535185463\n",
      "- Validation Loss: 25.235860566835147\n",
      "\n",
      "Epoch 68\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1553299033279235\n",
      "- Validation Loss: 25.092982317950273\n",
      "\n",
      "Epoch 69\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.15496047090971152\n",
      "- Validation Loss: 25.197282816912676\n",
      "\n",
      "Epoch 70\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.15543637877932642\n",
      "- Validation Loss: 25.622569006842536\n",
      "\n",
      "Epoch 71\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.15394771007340102\n",
      "- Validation Loss: 25.806627015809756\n",
      "\n",
      "Epoch 72\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1538742462764508\n",
      "- Validation Loss: 25.488528844472523\n",
      "\n",
      "Epoch    72: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 73\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1506785616190214\n",
      "- Validation Loss: 25.330277881106817\n",
      "\n",
      "Epoch 74\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.14751543274884407\n",
      "- Validation Loss: 25.79031622087633\n",
      "\n",
      "Epoch 75\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.14910167244536587\n",
      "- Validation Loss: 26.3659245259053\n",
      "\n",
      "Epoch 76\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.14663162495863544\n",
      "- Validation Loss: 25.749980952288652\n",
      "\n",
      "Epoch    76: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 77\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.14679575714805007\n",
      "- Validation Loss: 26.07058213208173\n",
      "\n",
      "Epoch 78\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1470581881082907\n",
      "- Validation Loss: 25.741751387312604\n",
      "\n",
      "Epoch 79\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.1462045428439842\n",
      "- Validation Loss: 25.825216654184704\n",
      "\n",
      "Epoch 80\n",
      "Training & Validating ----------------\n",
      "- Training Loss: 0.14804795357876785\n",
      "- Validation Loss: 25.256222364064808\n",
      "\n",
      "Epoch    80: reducing learning rate of group 0 to 1.9531e-06.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "batch_size = 32\n",
    "city = 'palo-alto'\n",
    "model_dim = 128\n",
    "conv_depth = 32\n",
    "kernel_size = 4\n",
    "conv_padding = 3\n",
    "n_heads = 8\n",
    "linear_dim = 128 # [32, 128]\n",
    "dropout_rate = 0.1 # [0, 0.5]\n",
    "encoder_layers = 4\n",
    "num_iters = 80 # [50, 100]\n",
    "learning_rate = 0.002 # [0.001, 0.005]\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 3\n",
    "regress_steps = 4\n",
    "\n",
    "palo_net, palo_loss = train(\n",
    "    batch_size, \n",
    "    city, \n",
    "    model_dim,\n",
    "    conv_depth,\n",
    "    kernel_size,\n",
    "    conv_padding,\n",
    "    n_heads,\n",
    "    linear_dim,\n",
    "    dropout_rate,\n",
    "    encoder_layers,\n",
    "    regress_steps,\n",
    "    num_iters,\n",
    "    learning_rate,\n",
    "    factor,\n",
    "    patience,\n",
    "    device=DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
