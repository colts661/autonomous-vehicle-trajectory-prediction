{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8225acbc",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "\n",
    "Upon building the final model we use for the result of the competition, we consulted [TensorFlow tutorials](https://www.tensorflow.org/text/tutorials/transformer) and PyTorch tutorials with practical examples such as [Language Modeling](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) and [Language Translation](https://pytorch.org/tutorials/beginner/translation_transformer.html.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a3d7322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e3292",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6db7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def transform_data(np_data, bch_id):\n",
    "    df = pd.DataFrame(np_data[bch_id], columns = ['x','y'])\n",
    "    df['x_vel'] = np.gradient(df.x)\n",
    "    df['y_vel'] = np.gradient(df.y)\n",
    "    df['vel'] = np.sqrt(df.x_vel**2 + df.y_vel**2)\n",
    "    df['x_acc'] = np.gradient(df.x_vel)\n",
    "    df['y_acc'] = np.gradient(df.y_vel)\n",
    "    df['acc'] = np.gradient(df.vel)\n",
    "    tangent = np.array([1/df.vel]*2).T * np.array([df.x_vel, df.y_vel]).T\n",
    "    df['curvature'] = np.abs(df.x_acc * df.y_vel - df.x_vel * df.y_acc) / (df.vel)**3\n",
    "    out = df[['x', 'y', 'curvature']]\n",
    "    return out.to_numpy()\n",
    "\n",
    "\n",
    "def rotate(X, startpoint, endpoint, default_angle):\n",
    "    \n",
    "    # Find the slope of the path\n",
    "    dx = X[:, endpoint, 0] - X[:, startpoint, 0]\n",
    "    dy = X[:, endpoint, 1] - X[:, startpoint, 1]\n",
    "    \n",
    "    # Convert theta to degree in the range(0, 360)\n",
    "    theta = np.arctan2(dy, dx)\n",
    "    angle = np.degrees(theta)\n",
    "    angle[angle < 0] += 360\n",
    "    \n",
    "    # Generate the degree we want to rotate by and convert back to theta\n",
    "    rotate_degree = -1 * (angle - default_angle)\n",
    "    rotate_theta = np.deg2rad(rotate_degree)\n",
    "    \n",
    "    # Reshape the array from [4, batchsize] to [batchsize, 2, 2]\n",
    "    rot = np.array([np.cos(rotate_theta), -np.sin(rotate_theta),\n",
    "                np.sin(rotate_theta), np.cos(rotate_theta)])\n",
    "    rot = rot.T.reshape(-1, 2, 2)\n",
    "    \n",
    "    return rot\n",
    "\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None, normalized=False):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.normalized = normalized\n",
    "        self.split = split\n",
    "\n",
    "        self.inputs, self.outputs = self.get_city_trajectories(city=city, split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == 'train':\n",
    "        \n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "\n",
    "#             if self.transform:\n",
    "#                 data = self.transform(data)\n",
    "\n",
    "            return data\n",
    "        \n",
    "        return self.inputs[idx]\n",
    "    \n",
    "    def get_city_trajectories(self, city=\"palo-alto\", split=\"train\"):\n",
    "        assert city in cities and split in splits\n",
    "\n",
    "        # get input\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "        # store input starting positions and rotation matrix\n",
    "        start_pos = inputs[:, 0, :].copy()\n",
    "        rotate_factor = rotate(inputs, 0, 49, 30)\n",
    "        max_factor = inputs.max(axis=1)\n",
    "        \n",
    "#         print(inputs.reshape(-1, 2).mean(axis=0))\n",
    "#         print(inputs.reshape(-1, 2).std(axis=0))\n",
    "\n",
    "        # normalize inputs (translation + rotation)\n",
    "        if self.normalized:\n",
    "            for i in range(len(inputs)):\n",
    "                inputs[i] -= start_pos[i, :]\n",
    "                \n",
    "            for i in range(len(inputs)):\n",
    "                inputs[i] = inputs[i] @ rotate_factor[i].T\n",
    "            \n",
    "            max_factor = inputs.max(axis=1)\n",
    "            \n",
    "#             for i in range(len(inputs)):\n",
    "#                 inputs[i] = inputs[i] / max_factor[i]\n",
    "\n",
    "        # get output\n",
    "        outputs = None\n",
    "        if split == \"train\":  # get and normalize outputs\n",
    "            f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "            outputs = pickle.load(open(f_out, \"rb\"))\n",
    "            outputs = np.asarray(outputs)\n",
    "            if self.normalized:\n",
    "                for i in range(len(inputs)):\n",
    "                    outputs[i] -= start_pos[i, :]\n",
    "                    \n",
    "                for i in range(len(inputs)):\n",
    "                    outputs[i] = outputs[i] @ rotate_factor[i].T\n",
    "                \n",
    "#                 for i in range(len(inputs)):\n",
    "#                     outputs[i] = outputs[i] / max_factor[i]\n",
    "        \n",
    "#             print(inputs.shape)\n",
    "#             print(outputs.shape)\n",
    "        \n",
    "            # Adding curvature as features\n",
    "            if self.transform:\n",
    "#                 print(inputs.shape)\n",
    "#                 print(outputs.shape)\n",
    "                inputs = np.array([transform_data(inputs, i) for i in range(len(inputs))])\n",
    "#                 print(inputs.shape)\n",
    "\n",
    "        self.start_pos = start_pos\n",
    "        self.rotate_matrix = rotate_factor # np.linalg.inv(rot[i].T) to reverse back\n",
    "        \n",
    "        if self.normalized:\n",
    "            self.n_max = max_factor\n",
    "\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf5059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        super(TotalDataset, self).__init__()\n",
    "        self.cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "        self.split = split\n",
    "        self.datasets = [ArgoverseDataset(c, split=split, normalized=True) for c in self.cities]\n",
    "        self.sizes = [len(data) for data in self.datasets]\n",
    "        self.cumu_sizes = [0] + np.cumsum(self.sizes).tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return sum(self.sizes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # compute combined idx\n",
    "        for i, cumu in enumerate(self.cumu_sizes):\n",
    "            if cumu <= idx < self.cumu_sizes[i+1]:\n",
    "                ix = idx - cumu\n",
    "                dataset = self.datasets[i]\n",
    "\n",
    "        if self.split == 'train':\n",
    "            return dataset.inputs[ix], dataset.outputs[ix]     \n",
    "        return dataset.inputs[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55edaccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203816, [43041, 55029, 43544, 24465, 25744, 11993])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initalize TOTAL DATASET\n",
    "total_dataset = TotalDataset('train')\n",
    "len(total_dataset), total_dataset.sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ee1e4",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ea3c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da5b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multi-head self-attention module'''\n",
    "    def __init__(self, D, H):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.H = H # number of heads\n",
    "        self.D = D # dimension\n",
    "        \n",
    "        self.wq = nn.Linear(D, D*H)\n",
    "        self.wk = nn.Linear(D, D*H)\n",
    "        self.wv = nn.Linear(D, D*H)\n",
    "\n",
    "        self.dense = nn.Linear(D*H, D)\n",
    "\n",
    "    def concat_heads(self, x):\n",
    "        B, H, S, D = x.shape\n",
    "        x = x.permute((0, 2, 1, 3)).contiguous() \n",
    "        x = x.reshape((B, S, H*D))\n",
    "        return x\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, S, D_H = x.shape\n",
    "        x = x.reshape(B, S, self.H, self.D)\n",
    "        x = x.permute((0, 2, 1, 3))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.D)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            attention_scores += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n",
    "        scaled_attention = torch.matmul(attention_weights, v)\n",
    "        concat_attention = self.concat_heads(scaled_attention)\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd474fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encodings\n",
    "def get_angles(pos, i, D):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(D, position=60, dim=3, device=device):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(D)[np.newaxis, :],\n",
    "                            D)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    if dim == 3:\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    elif dim == 4:\n",
    "        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n",
    "    return torch.tensor(pos_encoding, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d86646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size, device=device):\n",
    "    mask = torch.ones((size, size), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60a24212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n",
    "        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n",
    "        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n",
    "        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.mha = MultiHeadAttention(D, H)\n",
    "\n",
    "\n",
    "    def forward(self, x, look_ahead_mask):\n",
    "        \n",
    "        attn, attn_weights = self.mha(x, look_ahead_mask)\n",
    "        attn = self.dropout1(attn)\n",
    "        attn = self.layernorm1(attn + x)\n",
    "\n",
    "        mlp_act = torch.relu(self.mlp_hidden(attn))\n",
    "        mlp_act = self.mlp_out(mlp_act)\n",
    "        mlp_act = self.dropout2(mlp_act)\n",
    "        \n",
    "        output = self.layernorm2(mlp_act + attn)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcba1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Transformer Encoder\n",
    "    '''\n",
    "    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features,\n",
    "                 out_features, dropout_rate, batch_size, kernel_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.sqrt_D = torch.tensor(math.sqrt(D))\n",
    "        self.num_layers = num_layers\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(inp_features, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, D),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(50*D, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, hidden_mlp_dim),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(hidden_mlp_dim, out_features)\n",
    "        )\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(D)\n",
    "        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim, \n",
    "                                        dropout_rate=dropout_rate\n",
    "                                       ) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, S, D = x.shape\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        x *= self.sqrt_D\n",
    "        \n",
    "        x += self.pos_encoding[:, :S, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block = self.dec_layers[i](x=x,\n",
    "                                          look_ahead_mask=mask)\n",
    "            attention_weights['decoder_layer{}'.format(i + 1)] = block\n",
    "        \n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        return x, attention_weights\n",
    "    \n",
    "    def auto_regressor(self, x, mask, step):\n",
    "        \n",
    "        B, S, D = x.shape\n",
    "        new_inputs = torch.clone(x)\n",
    "        temp_pred, atn = self.forward(new_inputs, mask)\n",
    "        temp_pred = temp_pred.reshape(B, -1, 2)\n",
    "        new_inputs = torch.cat((new_inputs, temp_pred), 1)\n",
    "        \n",
    "        \n",
    "        for idx in range(step, 60, step):\n",
    "            train_inputs = new_inputs[:, idx:idx+50, :]\n",
    "            \n",
    "            starting_pos = torch.unsqueeze(train_inputs[:, 0, :], dim=1)\n",
    "            Q = torch.from_numpy(rotate(train_inputs.cpu().detach().numpy(), 0, 9, 30)).to(device)\n",
    "            trans_inputs = torch.matmul((train_inputs - starting_pos),\n",
    "                                        torch.transpose(Q, 1, 2))\n",
    "            \n",
    "            temp_pred, attention = self.forward(train_inputs, mask)\n",
    "            temp_pred = temp_pred.reshape(B, -1, 2)\n",
    "            temp_pred = (torch.matmul(temp_pred, Q) + starting_pos)\n",
    "            new_inputs = torch.cat((new_inputs, temp_pred), 1)\n",
    "            \n",
    "        return new_inputs[:, 50:].reshape(B, -1), attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57610ecc",
   "metadata": {},
   "source": [
    "## Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f002463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, city, split, num_layers, D, H, hidden_mlp_dim, \n",
    "          inp_features, out_features, dropout_rate, n_epochs, learning_rate, factor, patience,\n",
    "          step, kernel_size):\n",
    "    \n",
    "    # Create the training/validation set\n",
    "    train_dataset = ArgoverseDataset(city=city, split=split, transform=False, normalized=True)\n",
    "    train_sz = int(len(train_dataset) * 0.9)\n",
    "    val_sz = len(train_dataset) - train_sz\n",
    "    train_loader, val_loader = torch.utils.data.random_split(train_dataset, [train_sz, val_sz])\n",
    "    train_loader = DataLoader(train_loader, batch_size=batch_size, drop_last=True)\n",
    "    val_loader = DataLoader(val_loader, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    # Initialize the transformer/optimizer/loss function\n",
    "    transformer = Transformer(num_layers=num_layers, D=D, H=H, hidden_mlp_dim=hidden_mlp_dim,\n",
    "                          inp_features=inp_features, out_features=out_features,\n",
    "                          dropout_rate=dropout_rate, batch_size=batch_size,\n",
    "                          kernel_size=kernel_size).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate) \n",
    "    loss_function = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor,\n",
    "                                  patience=patience, verbose=True) \n",
    "    \n",
    "    # Print out how many parameters to train\n",
    "    param_sizes = [p.numel() for p in transformer.parameters()]\n",
    "    print(f\"number of weight/biases matrices: {len(param_sizes)} \"\n",
    "          f\"for a total of {np.sum(param_sizes)} parameters \")\n",
    "    \n",
    "    avg_train_loss, avg_val_loss = [], []\n",
    "    train_time, elapsed_time = [], []\n",
    "    \n",
    "    # Start training\n",
    "    for epoch in tqdm(list(range(n_epochs))):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print('Training & Validating ', end='')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_loss, val_loss = [], []\n",
    "        \n",
    "        # Training set\n",
    "        for batches, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            # Track progress\n",
    "            if (batches + 1) % 20 == 0:\n",
    "                print('-', end='')\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            S = X.shape[1]\n",
    "            mask = create_look_ahead_mask(S)\n",
    "            out, _ = transformer(X, mask) # .auto_regressor(X, mask, step)\n",
    "            \n",
    "#             print(out.shape)\n",
    "#             print(y.reshape(batch_size, -1).shape)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_function(out, y.reshape(batch_size, -1)) # y.reshape(batch_size, -1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        print()\n",
    "        avg_train = np.mean(train_loss)\n",
    "        avg_train_loss.append(avg_train)\n",
    "        \n",
    "        # End the time\n",
    "        end_train_time = time.time()\n",
    "        train_time.append(end_train_time - start_time)\n",
    "        \n",
    "        # Evaluate on val set\n",
    "        with torch.no_grad():\n",
    "            for batches, (X, y) in enumerate(val_loader):\n",
    "                X = X.to(device).float()\n",
    "                y = y.to(device).float()\n",
    "\n",
    "                S = X.shape[1]\n",
    "                mask = create_look_ahead_mask(S)\n",
    "                out, _ = transformer(X, mask) # .auto_regressor(X, mask, step)\n",
    "                loss = loss_function(out, y.reshape(batch_size, -1)) # y.reshape(batch_size, -1)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "            avg_val = np.mean(val_loss)\n",
    "            avg_val_loss.append(avg_val)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time.append(end_time - start_time)\n",
    "\n",
    "        print(f'- Training Loss: {avg_train}\\n- Validation Loss: {avg_val}')\n",
    "        print(f'- Train Time: {sum(train_time)}\\n- Elapsed Time: {sum(elapsed_time)}\\n')\n",
    "        \n",
    "        scheduler.step(avg_val)\n",
    "        \n",
    "    return transformer, (avg_train_loss, avg_val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba200856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_total(total_dataset, batch_size, num_layers, D, H, hidden_mlp_dim, \n",
    "          inp_features, out_features, dropout_rate, n_epochs, learning_rate, factor, patience,\n",
    "          step, kernel_size):\n",
    "    \n",
    "    # Create the training/validation set\n",
    "    train_sz = int(len(total_dataset) * 0.9)\n",
    "    val_sz = len(total_dataset) - train_sz\n",
    "    train_loader, val_loader = torch.utils.data.random_split(total_dataset, [train_sz, val_sz])\n",
    "    train_loader = DataLoader(train_loader, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "    val_loader = DataLoader(val_loader, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    # Initialize the transformer/optimizer/loss function\n",
    "    transformer = Transformer(num_layers=num_layers, D=D, H=H, hidden_mlp_dim=hidden_mlp_dim,\n",
    "                          inp_features=inp_features, out_features=out_features,\n",
    "                          dropout_rate=dropout_rate, batch_size=batch_size,\n",
    "                          kernel_size=kernel_size).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate) \n",
    "    loss_function = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor,\n",
    "                                  patience=patience, verbose=True) \n",
    "    \n",
    "    # Print out how many parameters to train\n",
    "    param_sizes = [p.numel() for p in transformer.parameters()]\n",
    "    print(f\"number of weight/biases matrices: {len(param_sizes)} \"\n",
    "          f\"for a total of {np.sum(param_sizes)} parameters \")\n",
    "    \n",
    "    avg_train_loss, avg_val_loss = [], []\n",
    "    train_time, elapsed_time = [], []\n",
    "    best_val_score = float('inf')\n",
    "    \n",
    "    # Start training\n",
    "    for epoch in tqdm(list(range(n_epochs))):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print('Training & Validating ', end='')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_loss, val_loss = [], []\n",
    "        \n",
    "        # Training set\n",
    "        for batches, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            # Track progress\n",
    "            if (batches + 1) % 120 == 0:\n",
    "                print('-', end='')\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            S = X.shape[1]\n",
    "            mask = create_look_ahead_mask(S)\n",
    "            out, _ = transformer(X, mask) # .auto_regressor(X, mask, step)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_function(out, y.reshape(batch_size, -1)) # y.reshape(batch_size, -1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        print()\n",
    "        avg_train = np.mean(train_loss)\n",
    "        avg_train_loss.append(avg_train)\n",
    "        \n",
    "        # End the time\n",
    "        end_train_time = time.time()\n",
    "        train_time.append(end_train_time - start_time)\n",
    "        \n",
    "        # Evaluate on val set\n",
    "        with torch.no_grad():\n",
    "            for batches, (X, y) in enumerate(val_loader):\n",
    "                X = X.to(device).float()\n",
    "                y = y.to(device).float()\n",
    "\n",
    "                S = X.shape[1]\n",
    "                mask = create_look_ahead_mask(S)\n",
    "                out, _ = transformer(X, mask) # .auto_regressor(X, mask, step)\n",
    "                loss = loss_function(out, y.reshape(batch_size, -1)) # y.reshape(batch_size, -1)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "            avg_val = np.mean(val_loss)\n",
    "            avg_val_loss.append(avg_val)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time.append(end_time - start_time)\n",
    "\n",
    "        print(f'- Training Loss: {avg_train}\\n- Validation Loss: {avg_val}')\n",
    "        print(f'- Train Time: {sum(train_time)}\\n- Elapsed Time: {sum(elapsed_time)}\\n')\n",
    "        \n",
    "        scheduler.step(avg_val)\n",
    "        \n",
    "        # save better model\n",
    "        if avg_val < best_val_score:\n",
    "            best_val_score = avg_val\n",
    "            torch.save(transformer, f'total_model.pt')\n",
    "        \n",
    "    return transformer, (avg_train_loss, avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59a82219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_results(city, split, batch_size, model, idx):\n",
    "    train_dataset = ArgoverseDataset(city = city, split = split, transform=False, normalized=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (X, y) in train_loader:\n",
    "            X = X.to(device).float()\n",
    "            S = X.shape[1]\n",
    "            mask = create_look_ahead_mask(S)\n",
    "            \n",
    "            output = model(X, mask)[0].reshape(batch_size, -1, 2)\n",
    "        \n",
    "            break\n",
    "    \n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(output.shape)\n",
    "    X = X.cpu()\n",
    "    output = output.cpu()\n",
    "    \n",
    "    x_jump = train_dataset.start_pos[idx, 0]\n",
    "    y_jump = train_dataset.start_pos[idx, 0]\n",
    "    rot = train_dataset.rotate_matrix[idx].T\n",
    "    X = X[idx] @ np.linalg.inv(rot) + train_dataset.start_pos[idx]\n",
    "    y = y[idx] @ np.linalg.inv(rot) + train_dataset.start_pos[idx]\n",
    "    output = output[idx] @ np.linalg.inv(rot) + train_dataset.start_pos[idx]\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], label='seed')\n",
    "    plt.scatter(y[:, 0], y[:, 1], label='ground truth')\n",
    "    plt.scatter(output[:, 0], output[:, 1], label='prediction')\n",
    "    plt.title(f'Random Sample From {city}_{split} Projectile Visualization')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bfac29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(test_loader, batch_sz, model):\n",
    "    '''\n",
    "    Remember to use test_dataset stats, NOT train_dataset\n",
    "    '''\n",
    "    count_row = 0\n",
    "    out = []\n",
    "\n",
    "    for X in test_loader:\n",
    "        if len(X) != batch_sz:\n",
    "            print(len(X))\n",
    "            to_fill = np.zeros([batch_sz-len(X), 50, 2])\n",
    "            X = torch.from_numpy(np.append(X, to_fill, axis=0))\n",
    "\n",
    "        X = X.to(device).float()\n",
    "    \n",
    "        S = X.shape[1]\n",
    "        mask = create_look_ahead_mask(S)\n",
    "\n",
    "        pred = model(X, mask)[0].reshape(batch_size, -1, 2).cpu().detach().numpy()\n",
    "\n",
    "        for i in range(batch_sz):\n",
    "            if count_row >= len(test_dataset):\n",
    "                break\n",
    "\n",
    "            rotation =  test_dataset.rotate_matrix[count_row].T\n",
    "            pred[i] = pred[i] @ np.linalg.inv(rotation)\n",
    "            pred[i] = pred[i] + test_dataset.start_pos[count_row, : ]\n",
    "                \n",
    "            out.append(pred[i])\n",
    "            count_row += 1 \n",
    "\n",
    "    out = np.array(out).reshape(len(test_dataset), -1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e8b4d",
   "metadata": {},
   "source": [
    "### Training Aggregated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c5ed99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of weight/biases matrices: 82 for a total of 284888 parameters \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3328abd806bf40afa680aa2b6d3dd005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 53.94105208743642\n",
      "- Validation Loss: 34.111977682173624\n",
      "- Train Time: 100.11404538154602\n",
      "- Elapsed Time: 102.7796220779419\n",
      "\n",
      "Epoch 2\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 29.208138301167885\n",
      "- Validation Loss: 27.15102251820594\n",
      "- Train Time: 201.29149627685547\n",
      "- Elapsed Time: 206.69629311561584\n",
      "\n",
      "Epoch 3\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 26.48135671554041\n",
      "- Validation Loss: 24.872037895070683\n",
      "- Train Time: 301.95279836654663\n",
      "- Elapsed Time: 309.80698323249817\n",
      "\n",
      "Epoch 4\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 23.52547160812133\n",
      "- Validation Loss: 26.950462123882847\n",
      "- Train Time: 402.87421441078186\n",
      "- Elapsed Time: 413.1440258026123\n",
      "\n",
      "Epoch 5\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 22.547018451867032\n",
      "- Validation Loss: 20.897719299268424\n",
      "- Train Time: 502.2512261867523\n",
      "- Elapsed Time: 514.9260745048523\n",
      "\n",
      "Epoch 6\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 22.321968576247595\n",
      "- Validation Loss: 24.096814695394265\n",
      "- Train Time: 599.6811714172363\n",
      "- Elapsed Time: 614.6464242935181\n",
      "\n",
      "Epoch 7\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 21.383709156421705\n",
      "- Validation Loss: 27.292960120447027\n",
      "- Train Time: 697.0063500404358\n",
      "- Elapsed Time: 714.3795471191406\n",
      "\n",
      "Epoch 8\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 22.564483932573648\n",
      "- Validation Loss: 19.746099198389352\n",
      "- Train Time: 794.5893127918243\n",
      "- Elapsed Time: 814.276210308075\n",
      "\n",
      "Epoch 9\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 21.39818910319574\n",
      "- Validation Loss: 20.093935599866903\n",
      "- Train Time: 892.8588306903839\n",
      "- Elapsed Time: 914.9303534030914\n",
      "\n",
      "Epoch 10\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 27.02379827431175\n",
      "- Validation Loss: 23.485657230113286\n",
      "- Train Time: 989.0542995929718\n",
      "- Elapsed Time: 1013.4820268154144\n",
      "\n",
      "Epoch 11\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 23.499073851882205\n",
      "- Validation Loss: 24.7460170077078\n",
      "- Train Time: 1088.08389210701\n",
      "- Elapsed Time: 1114.9788935184479\n",
      "\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 20.142742474513376\n",
      "- Validation Loss: 24.82763372277314\n",
      "- Train Time: 1186.8727612495422\n",
      "- Elapsed Time: 1216.209790945053\n",
      "\n",
      "Epoch 13\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 19.727350046064\n",
      "- Validation Loss: 19.25070277774859\n",
      "- Train Time: 1285.2788348197937\n",
      "- Elapsed Time: 1316.968646287918\n",
      "\n",
      "Epoch 14\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 19.302355423725658\n",
      "- Validation Loss: 18.880894842387747\n",
      "- Train Time: 1386.1719007492065\n",
      "- Elapsed Time: 1420.3017218112946\n",
      "\n",
      "Epoch 15\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 19.156404226872777\n",
      "- Validation Loss: 20.009370455202067\n",
      "- Train Time: 1484.5925462245941\n",
      "- Elapsed Time: 1521.1281321048737\n",
      "\n",
      "Epoch 16\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 18.788064859583923\n",
      "- Validation Loss: 20.286021286586546\n",
      "- Train Time: 1591.805394411087\n",
      "- Elapsed Time: 1630.9192810058594\n",
      "\n",
      "Epoch 17\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 19.0297557165468\n",
      "- Validation Loss: 18.381499498895128\n",
      "- Train Time: 1699.3066086769104\n",
      "- Elapsed Time: 1740.9497246742249\n",
      "\n",
      "Epoch 18\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 18.424620021329588\n",
      "- Validation Loss: 18.172724222986954\n",
      "- Train Time: 1806.8404657840729\n",
      "- Elapsed Time: 1851.1338925361633\n",
      "\n",
      "Epoch 19\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 18.354725515351625\n",
      "- Validation Loss: 17.70521060550738\n",
      "- Train Time: 1914.8751056194305\n",
      "- Elapsed Time: 1961.6407413482666\n",
      "\n",
      "Epoch 20\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 18.07215370078729\n",
      "- Validation Loss: 18.962081910679174\n",
      "- Train Time: 2023.018034696579\n",
      "- Elapsed Time: 2072.2420852184296\n",
      "\n",
      "Epoch 21\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 18.16613405123509\n",
      "- Validation Loss: 17.545417034401083\n",
      "- Train Time: 2124.9988605976105\n",
      "- Elapsed Time: 2176.6721625328064\n",
      "\n",
      "Epoch 22\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 17.85257812837413\n",
      "- Validation Loss: 17.542826767987425\n",
      "- Train Time: 2227.511951446533\n",
      "- Elapsed Time: 2281.6410613059998\n",
      "\n",
      "Epoch 23\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 17.81304485142356\n",
      "- Validation Loss: 17.83440577234112\n",
      "- Train Time: 2329.9297540187836\n",
      "- Elapsed Time: 2386.462728500366\n",
      "\n",
      "Epoch 24\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 17.68540776524607\n",
      "- Validation Loss: 17.51490211786714\n",
      "- Train Time: 2428.8586554527283\n",
      "- Elapsed Time: 2487.8957216739655\n",
      "\n",
      "Epoch 25\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 17.559018434383447\n",
      "- Validation Loss: 17.06911844502455\n",
      "- Train Time: 2530.046531200409\n",
      "- Elapsed Time: 2591.505440711975\n",
      "\n",
      "Epoch 26\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 17.45108286334515\n",
      "- Validation Loss: 16.876257416587205\n",
      "- Train Time: 2628.1811316013336\n",
      "- Elapsed Time: 2692.1279413700104\n",
      "\n",
      "Epoch 27\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 27.77179491569841\n",
      "- Validation Loss: 65.43291747794962\n",
      "- Train Time: 2730.285739660263\n",
      "- Elapsed Time: 2796.6197452545166\n",
      "\n",
      "Epoch 28\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 46.34246105242674\n",
      "- Validation Loss: 27.92160243508201\n",
      "- Train Time: 2831.3270094394684\n",
      "- Elapsed Time: 2900.0907497406006\n",
      "\n",
      "Epoch 29\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 33.59000117705572\n",
      "- Validation Loss: 30.83217747856236\n",
      "- Train Time: 2931.9422867298126\n",
      "- Elapsed Time: 3003.0973420143127\n",
      "\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 30\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 24.906961909724714\n",
      "- Validation Loss: 21.271340349935137\n",
      "- Train Time: 3031.6878848075867\n",
      "- Elapsed Time: 3105.2935707569122\n",
      "\n",
      "Epoch 31\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 20.855658335113393\n",
      "- Validation Loss: 17.53115880414375\n",
      "- Train Time: 3130.983716249466\n",
      "- Elapsed Time: 3207.0853865146637\n",
      "\n",
      "Epoch 32\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 17.79990121191894\n",
      "- Validation Loss: 17.024122581541913\n",
      "- Train Time: 3229.3121082782745\n",
      "- Elapsed Time: 3307.816085100174\n",
      "\n",
      "Epoch    32: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 33\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.838379638975347\n",
      "- Validation Loss: 16.43114612762283\n",
      "- Train Time: 3327.5005974769592\n",
      "- Elapsed Time: 3408.402577638626\n",
      "\n",
      "Epoch 34\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.595816561997726\n",
      "- Validation Loss: 16.26517729564283\n",
      "- Train Time: 3425.699945449829\n",
      "- Elapsed Time: 3508.966475009918\n",
      "\n",
      "Epoch 35\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.46992801014394\n",
      "- Validation Loss: 16.370429145465106\n",
      "- Train Time: 3522.7664635181427\n",
      "- Elapsed Time: 3608.3811645507812\n",
      "\n",
      "Epoch 36\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.327087603327453\n",
      "- Validation Loss: 16.171681424356855\n",
      "- Train Time: 3622.119175195694\n",
      "- Elapsed Time: 3710.1798214912415\n",
      "\n",
      "Epoch 37\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.231752863649046\n",
      "- Validation Loss: 15.959091538153354\n",
      "- Train Time: 3721.112364292145\n",
      "- Elapsed Time: 3811.676356077194\n",
      "\n",
      "Epoch 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.14863223731726\n",
      "- Validation Loss: 15.919437372459555\n",
      "- Train Time: 3819.2717378139496\n",
      "- Elapsed Time: 3912.23441696167\n",
      "\n",
      "Epoch 39\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.071488157819388\n",
      "- Validation Loss: 16.13752105775869\n",
      "- Train Time: 3919.211638689041\n",
      "- Elapsed Time: 4014.590362071991\n",
      "\n",
      "Epoch 40\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 16.01641934348916\n",
      "- Validation Loss: 16.479922321607482\n",
      "- Train Time: 4019.4878692626953\n",
      "- Elapsed Time: 4117.308619976044\n",
      "\n",
      "Epoch 41\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.946612277982622\n",
      "- Validation Loss: 16.895273418546473\n",
      "- Train Time: 4118.601330757141\n",
      "- Elapsed Time: 4218.920133590698\n",
      "\n",
      "Epoch    41: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 42\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.572360513681144\n",
      "- Validation Loss: 15.561515466222223\n",
      "- Train Time: 4215.453132390976\n",
      "- Elapsed Time: 4318.196531534195\n",
      "\n",
      "Epoch 43\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.534024428173286\n",
      "- Validation Loss: 15.474149424324995\n",
      "- Train Time: 4315.622015476227\n",
      "- Elapsed Time: 4420.997395277023\n",
      "\n",
      "Epoch 44\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.488999922501817\n",
      "- Validation Loss: 15.464150059897944\n",
      "- Train Time: 4421.025166511536\n",
      "- Elapsed Time: 4528.9824459552765\n",
      "\n",
      "Epoch 45\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.450124377731834\n",
      "- Validation Loss: 15.441953663556081\n",
      "- Train Time: 4524.856043100357\n",
      "- Elapsed Time: 4635.2418031692505\n",
      "\n",
      "Epoch 46\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.419411354354438\n",
      "- Validation Loss: 15.407540840922662\n",
      "- Train Time: 4628.885096311569\n",
      "- Elapsed Time: 4741.667598724365\n",
      "\n",
      "Epoch 47\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.389755158347844\n",
      "- Validation Loss: 15.284717366380512\n",
      "- Train Time: 4733.798225164413\n",
      "- Elapsed Time: 4849.104569196701\n",
      "\n",
      "Epoch 48\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.349916694302369\n",
      "- Validation Loss: 15.211832716779888\n",
      "- Train Time: 4837.411380529404\n",
      "- Elapsed Time: 4955.164850950241\n",
      "\n",
      "Epoch 49\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.328960116726488\n",
      "- Validation Loss: 15.366325563604727\n",
      "- Train Time: 4936.505790948868\n",
      "- Elapsed Time: 5056.636989116669\n",
      "\n",
      "Epoch 50\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.30145274802512\n",
      "- Validation Loss: 15.202095903690505\n",
      "- Train Time: 5034.4182369709015\n",
      "- Elapsed Time: 5157.029479503632\n",
      "\n",
      "Epoch 51\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.287200963255914\n",
      "- Validation Loss: 15.26370693452703\n",
      "- Train Time: 5132.352742433548\n",
      "- Elapsed Time: 5257.285704135895\n",
      "\n",
      "Epoch 52\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.227049935637698\n",
      "- Validation Loss: 15.23009464275912\n",
      "- Train Time: 5229.479875326157\n",
      "- Elapsed Time: 5356.78746843338\n",
      "\n",
      "Epoch 53\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.212678418821666\n",
      "- Validation Loss: 15.253058938860143\n",
      "- Train Time: 5329.863281488419\n",
      "- Elapsed Time: 5459.787722349167\n",
      "\n",
      "Epoch    53: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 54\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 15.010308210448793\n",
      "- Validation Loss: 15.086677750701424\n",
      "- Train Time: 5436.2267825603485\n",
      "- Elapsed Time: 5568.647754430771\n",
      "\n",
      "Epoch 55\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.986787964727025\n",
      "- Validation Loss: 15.134494330148277\n",
      "- Train Time: 5541.12162566185\n",
      "- Elapsed Time: 5676.15250992775\n",
      "\n",
      "Epoch 56\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.970000267236781\n",
      "- Validation Loss: 15.026455259173172\n",
      "- Train Time: 5642.414488077164\n",
      "- Elapsed Time: 5779.954819202423\n",
      "\n",
      "Epoch 57\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.966943799243136\n",
      "- Validation Loss: 15.049747260861427\n",
      "- Train Time: 5741.375300645828\n",
      "- Elapsed Time: 5881.358411312103\n",
      "\n",
      "Epoch 58\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.941060984625487\n",
      "- Validation Loss: 15.06769158030456\n",
      "- Train Time: 5839.585746765137\n",
      "- Elapsed Time: 5981.966648817062\n",
      "\n",
      "Epoch 59\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.935081639742402\n",
      "- Validation Loss: 15.05042067263861\n",
      "- Train Time: 5938.197039842606\n",
      "- Elapsed Time: 6082.915997028351\n",
      "\n",
      "Epoch    59: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 60\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.818774653703953\n",
      "- Validation Loss: 14.944148595228135\n",
      "- Train Time: 6038.29039311409\n",
      "- Elapsed Time: 6185.430231809616\n",
      "\n",
      "Epoch 61\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.811191644522166\n",
      "- Validation Loss: 14.942199345654661\n",
      "- Train Time: 6135.955342054367\n",
      "- Elapsed Time: 6285.421438217163\n",
      "\n",
      "Epoch 62\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.799467255872859\n",
      "- Validation Loss: 14.977306009088672\n",
      "- Train Time: 6232.681349515915\n",
      "- Elapsed Time: 6384.455908060074\n",
      "\n",
      "Epoch 63\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.793996362110493\n",
      "- Validation Loss: 14.96233117805337\n",
      "- Train Time: 6332.166698217392\n",
      "- Elapsed Time: 6486.316963672638\n",
      "\n",
      "Epoch 64\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.781790020910078\n",
      "- Validation Loss: 14.930869426367417\n",
      "- Train Time: 6430.612347126007\n",
      "- Elapsed Time: 6587.1755464077\n",
      "\n",
      "Epoch 65\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.781732350858208\n",
      "- Validation Loss: 15.004491393671096\n",
      "- Train Time: 6529.579475641251\n",
      "- Elapsed Time: 6688.60173535347\n",
      "\n",
      "Epoch 66\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.763061482924643\n",
      "- Validation Loss: 14.91289268649599\n",
      "- Train Time: 6629.365794658661\n",
      "- Elapsed Time: 6790.787314891815\n",
      "\n",
      "Epoch 67\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.76441420280925\n",
      "- Validation Loss: 14.969335107683385\n",
      "- Train Time: 6728.074348449707\n",
      "- Elapsed Time: 6891.881760835648\n",
      "\n",
      "Epoch 68\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.75343660478898\n",
      "- Validation Loss: 14.901370652816581\n",
      "- Train Time: 6824.733194351196\n",
      "- Elapsed Time: 6990.90127658844\n",
      "\n",
      "Epoch 69\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.745597907403758\n",
      "- Validation Loss: 14.926715016365051\n",
      "- Train Time: 6924.618408918381\n",
      "- Elapsed Time: 7093.12419629097\n",
      "\n",
      "Epoch 70\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.741182311963803\n",
      "- Validation Loss: 14.885322332382202\n",
      "- Train Time: 7020.3360641002655\n",
      "- Elapsed Time: 7191.132241010666\n",
      "\n",
      "Epoch 71\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.734242856627317\n",
      "- Validation Loss: 14.934081381971732\n",
      "- Train Time: 7118.122473239899\n",
      "- Elapsed Time: 7291.388567209244\n",
      "\n",
      "Epoch 72\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.726546434198154\n",
      "- Validation Loss: 14.917024649164212\n",
      "- Train Time: 7216.749378919601\n",
      "- Elapsed Time: 7392.517418861389\n",
      "\n",
      "Epoch 73\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.715837837347015\n",
      "- Validation Loss: 14.938660392971158\n",
      "- Train Time: 7315.543452739716\n",
      "- Elapsed Time: 7493.669996738434\n",
      "\n",
      "Epoch    73: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 74\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.659806594262985\n",
      "- Validation Loss: 14.871647269470886\n",
      "- Train Time: 7415.962412595749\n",
      "- Elapsed Time: 7596.494191646576\n",
      "\n",
      "Epoch 75\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.654449369345024\n",
      "- Validation Loss: 14.8512833208408\n",
      "- Train Time: 7515.569743871689\n",
      "- Elapsed Time: 7698.460653543472\n",
      "\n",
      "Epoch 76\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.649636843407478\n",
      "- Validation Loss: 14.847722034034488\n",
      "- Train Time: 7612.7808084487915\n",
      "- Elapsed Time: 7798.0033276081085\n",
      "\n",
      "Epoch 77\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.648035582427951\n",
      "- Validation Loss: 14.88019530128383\n",
      "- Train Time: 7707.423678398132\n",
      "- Elapsed Time: 7895.025150060654\n",
      "\n",
      "Epoch 78\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.64361329562832\n",
      "- Validation Loss: 14.854304684033185\n",
      "- Train Time: 7806.966818332672\n",
      "- Elapsed Time: 7996.956224441528\n",
      "\n",
      "Epoch 79\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.641092508469773\n",
      "- Validation Loss: 14.85796931704635\n",
      "- Train Time: 7909.775385379791\n",
      "- Elapsed Time: 8102.3307492733\n",
      "\n",
      "Epoch    79: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 80\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.610470922693704\n",
      "- Validation Loss: 14.828586652593792\n",
      "- Train Time: 8018.917281866074\n",
      "- Elapsed Time: 8214.14673614502\n",
      "\n",
      "Epoch 81\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.604145767384281\n",
      "- Validation Loss: 14.864569432330581\n",
      "- Train Time: 8132.7966322898865\n",
      "- Elapsed Time: 8330.718398809433\n",
      "\n",
      "Epoch 82\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.604648056516094\n",
      "- Validation Loss: 14.846834598097411\n",
      "- Train Time: 8251.121055841446\n",
      "- Elapsed Time: 8451.702827215195\n",
      "\n",
      "Epoch 83\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.60026566473155\n",
      "- Validation Loss: 14.851916633312058\n",
      "- Train Time: 8365.842980384827\n",
      "- Elapsed Time: 8569.032397508621\n",
      "\n",
      "Epoch    83: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 84\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.583883804203493\n",
      "- Validation Loss: 14.829589701298648\n",
      "- Train Time: 8473.572467327118\n",
      "- Elapsed Time: 8679.5097053051\n",
      "\n",
      "Epoch 85\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.584193382409596\n",
      "- Validation Loss: 14.820388740713492\n",
      "- Train Time: 8582.738943576813\n",
      "- Elapsed Time: 8791.09108543396\n",
      "\n",
      "Epoch 86\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.582936167384226\n",
      "- Validation Loss: 14.821745533613289\n",
      "- Train Time: 8692.166779756546\n",
      "- Elapsed Time: 8903.540978193283\n",
      "\n",
      "Epoch 87\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.581573648379553\n",
      "- Validation Loss: 14.827423551547453\n",
      "- Train Time: 8797.5036444664\n",
      "- Elapsed Time: 9011.285274028778\n",
      "\n",
      "Epoch 88\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.580545699396753\n",
      "- Validation Loss: 14.819593097428855\n",
      "- Train Time: 8906.166776418686\n",
      "- Elapsed Time: 9122.54649567604\n",
      "\n",
      "Epoch    88: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 89\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.571305868597356\n",
      "- Validation Loss: 14.815336667516696\n",
      "- Train Time: 9016.28042769432\n",
      "- Elapsed Time: 9235.387513875961\n",
      "\n",
      "Epoch 90\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.570925256628634\n",
      "- Validation Loss: 14.818122868267995\n",
      "- Train Time: 9130.49444270134\n",
      "- Elapsed Time: 9352.312836408615\n",
      "\n",
      "Epoch 91\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.569445826586358\n",
      "- Validation Loss: 14.817832753343403\n",
      "- Train Time: 9243.674454927444\n",
      "- Elapsed Time: 9468.13722896576\n",
      "\n",
      "Epoch 92\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.569840162846898\n",
      "- Validation Loss: 14.82175153831266\n",
      "- Train Time: 9356.716180562973\n",
      "- Elapsed Time: 9583.87275838852\n",
      "\n",
      "Epoch    92: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 93\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.56508611343328\n",
      "- Validation Loss: 14.818260808410885\n",
      "- Train Time: 9467.848403453827\n",
      "- Elapsed Time: 9697.615191936493\n",
      "\n",
      "Epoch 94\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.564025826830175\n",
      "- Validation Loss: 14.816160741842017\n",
      "- Train Time: 9579.709055423737\n",
      "- Elapsed Time: 9812.17273736\n",
      "\n",
      "Epoch 95\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.564168366492419\n",
      "- Validation Loss: 14.81600096165759\n",
      "- Train Time: 9691.422949314117\n",
      "- Elapsed Time: 9926.639803886414\n",
      "\n",
      "Epoch    95: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 96\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.562388455543331\n",
      "- Validation Loss: 14.818717699380791\n",
      "- Train Time: 9802.746917963028\n",
      "- Elapsed Time: 10040.614357948303\n",
      "\n",
      "Epoch 97\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.562332627911378\n",
      "- Validation Loss: 14.818212231750008\n",
      "- Train Time: 9916.092714071274\n",
      "- Elapsed Time: 10156.646938085556\n",
      "\n",
      "Epoch 98\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.561809801256748\n",
      "- Validation Loss: 14.816126732706273\n",
      "- Train Time: 10028.523238420486\n",
      "- Elapsed Time: 10271.763634443283\n",
      "\n",
      "Epoch    98: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 99\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.559727084736515\n",
      "- Validation Loss: 14.817785887598241\n",
      "- Train Time: 10140.936190128326\n",
      "- Elapsed Time: 10386.837289571762\n",
      "\n",
      "Epoch 100\n",
      "Training & Validating -----------------------------------------------\n",
      "- Training Loss: 14.560167950845148\n",
      "- Validation Loss: 14.8166514889999\n",
      "- Train Time: 10253.202988386154\n",
      "- Elapsed Time: 10501.79778933525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "batch_size = 32 \n",
    "num_layers = 4 # The more of this, more easily to overfit\n",
    "D = 32 # DIMENSION                                             8\n",
    "H = 8 # NUMBER OF HEADS                                       8 \n",
    "hidden_mlp_dim = 64 # [32, 128]                               32\n",
    "inp_features = 2 \n",
    "out_features = 120                                         \n",
    "dropout_rate = 0 # [0, 0.5]\n",
    "n_epochs = 100 # [50, 100]\n",
    "learning_rate = 0.002 # [0.001, 0.005]\n",
    "factor = 0.5 # 0.1 ~ 0.99\n",
    "patience = 2\n",
    "step = 20  # (20)x2\n",
    "kernel_size = 3\n",
    "\n",
    "total_net, total_losses = train_total(total_dataset, batch_size, num_layers, D, H, hidden_mlp_dim, \n",
    "          inp_features, out_features, dropout_rate, n_epochs, learning_rate, factor, patience,\n",
    "                               step, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5722ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(total_net, 'best_total.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27c3b0",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c108fd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29843, [6325, 7971, 6361, 3671, 3829, 1686])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_dataset = TotalDataset(split='test')\n",
    "len(total_test_dataset), total_test_dataset.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ce2720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6325, 120)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'austin' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[0]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "austin_array = make_pred(test_loader, 32, total_net)\n",
    "austin_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcafd6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7971, 120)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'miami' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[1]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "miami_array = make_pred(test_loader, 32, total_net)\n",
    "miami_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0b095d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6361, 120)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'pittsburgh' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[2]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "pitts_array = make_pred(test_loader, 32, total_net)\n",
    "pitts_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e061700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3671, 120)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'dearborn' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[3]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "dearborn_array = make_pred(test_loader, 32, total_net)\n",
    "dearborn_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "505ae028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3829, 120)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'washington-dc' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[4]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "wash_array = make_pred(test_loader, 32, total_net)\n",
    "wash_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "625484a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1686, 120)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = 'palo-alto' \n",
    "\n",
    "test_dataset = total_test_dataset.datasets[5]\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "palo_array = make_pred(test_loader, 32, total_net)\n",
    "palo_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd7b70",
   "metadata": {},
   "source": [
    "## Write File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad715572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6325\n",
      "7971\n",
      "6361\n",
      "3671\n",
      "3829\n",
      "1686\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "cols = [['ID'] + ['v{}'.format(i) for i in range(120)]]\n",
    "\n",
    "with open('output.csv', 'w+') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    mywriter.writerows(cols)\n",
    "\n",
    "with open('output.csv', 'a') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(austin_array)):\n",
    "        temp = [np.append(['{}_austin'.format(i)], austin_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(miami_array)):\n",
    "        temp = [np.append(['{}_miami'.format(i)], miami_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(pitts_array)):\n",
    "        temp = [np.append(['{}_pittsburgh'.format(i)], pitts_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(dearborn_array)):\n",
    "        temp = [np.append(['{}_dearborn'.format(i)], dearborn_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(wash_array)):\n",
    "        temp = [np.append(['{}_washington-dc'.format(i)], wash_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(palo_array)):\n",
    "        temp = [np.append(['{}_palo-alto'.format(i)], palo_array[i])]\n",
    "        mywriter.writerows(temp)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
